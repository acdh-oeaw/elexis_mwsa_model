{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('train/english_nuig.tsv', 'rb') as en:\n",
    "    res = pd.read_csv(en, sep='\\t', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>squall</td>\n",
       "      <td>verb</td>\n",
       "      <td>blow in a squall</td>\n",
       "      <td>to cry out; to scream or cry violently, as a w...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>squall</td>\n",
       "      <td>verb</td>\n",
       "      <td>make high-pitched, whiney noises</td>\n",
       "      <td>to cry out; to scream or cry violently, as a w...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>squall</td>\n",
       "      <td>verb</td>\n",
       "      <td>utter a sudden loud cry</td>\n",
       "      <td>to cry out; to scream or cry violently, as a w...</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tumid</td>\n",
       "      <td>adjective</td>\n",
       "      <td>abnormally distended especially by fluids or gas</td>\n",
       "      <td>swelled, enlarged, or distended;</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tumid</td>\n",
       "      <td>adjective</td>\n",
       "      <td>abnormally distended especially by fluids or gas</td>\n",
       "      <td>rising above the level; protuberant.</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to make an attempt; to make an essay or a tria...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8333</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to present in words; to proffer; to make a pro...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to attempt; to undertake.</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8335</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to bid, as a price, reward, or wages;</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8336</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to put in opposition to; to manifest in an off...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8337 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1                                                 2  \\\n",
       "0     squall       verb                                  blow in a squall   \n",
       "1     squall       verb                  make high-pitched, whiney noises   \n",
       "2     squall       verb                           utter a sudden loud cry   \n",
       "3      tumid  adjective  abnormally distended especially by fluids or gas   \n",
       "4      tumid  adjective  abnormally distended especially by fluids or gas   \n",
       "...      ...        ...                                               ...   \n",
       "8332   offer       verb                                   mount or put up   \n",
       "8333   offer       verb                                   mount or put up   \n",
       "8334   offer       verb                                   mount or put up   \n",
       "8335   offer       verb                                   mount or put up   \n",
       "8336   offer       verb                                   mount or put up   \n",
       "\n",
       "                                                      3      4  \n",
       "0     to cry out; to scream or cry violently, as a w...   none  \n",
       "1     to cry out; to scream or cry violently, as a w...   none  \n",
       "2     to cry out; to scream or cry violently, as a w...  exact  \n",
       "3                      swelled, enlarged, or distended;  exact  \n",
       "4                  rising above the level; protuberant.   none  \n",
       "...                                                 ...    ...  \n",
       "8332  to make an attempt; to make an essay or a tria...   none  \n",
       "8333  to present in words; to proffer; to make a pro...   none  \n",
       "8334                          to attempt; to undertake.   none  \n",
       "8335              to bid, as a price, reward, or wages;   none  \n",
       "8336  to put in opposition to; to manifest in an off...   none  \n",
       "\n",
       "[8337 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def related(rel):\n",
    "    if rel in ['broader','narrower','exact']:\n",
    "        rel = 'related'\n",
    "    else:\n",
    "        rel = 'none'\n",
    "        \n",
    "    return rel\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[4] = res[4].map(lambda rel: related(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>squall</td>\n",
       "      <td>verb</td>\n",
       "      <td>blow in a squall</td>\n",
       "      <td>to cry out; to scream or cry violently, as a w...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>squall</td>\n",
       "      <td>verb</td>\n",
       "      <td>make high-pitched, whiney noises</td>\n",
       "      <td>to cry out; to scream or cry violently, as a w...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>squall</td>\n",
       "      <td>verb</td>\n",
       "      <td>utter a sudden loud cry</td>\n",
       "      <td>to cry out; to scream or cry violently, as a w...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tumid</td>\n",
       "      <td>adjective</td>\n",
       "      <td>abnormally distended especially by fluids or gas</td>\n",
       "      <td>swelled, enlarged, or distended;</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tumid</td>\n",
       "      <td>adjective</td>\n",
       "      <td>abnormally distended especially by fluids or gas</td>\n",
       "      <td>rising above the level; protuberant.</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to make an attempt; to make an essay or a tria...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8333</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to present in words; to proffer; to make a pro...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to attempt; to undertake.</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8335</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to bid, as a price, reward, or wages;</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8336</th>\n",
       "      <td>offer</td>\n",
       "      <td>verb</td>\n",
       "      <td>mount or put up</td>\n",
       "      <td>to put in opposition to; to manifest in an off...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8337 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1                                                 2  \\\n",
       "0     squall       verb                                  blow in a squall   \n",
       "1     squall       verb                  make high-pitched, whiney noises   \n",
       "2     squall       verb                           utter a sudden loud cry   \n",
       "3      tumid  adjective  abnormally distended especially by fluids or gas   \n",
       "4      tumid  adjective  abnormally distended especially by fluids or gas   \n",
       "...      ...        ...                                               ...   \n",
       "8332   offer       verb                                   mount or put up   \n",
       "8333   offer       verb                                   mount or put up   \n",
       "8334   offer       verb                                   mount or put up   \n",
       "8335   offer       verb                                   mount or put up   \n",
       "8336   offer       verb                                   mount or put up   \n",
       "\n",
       "                                                      3        4  \n",
       "0     to cry out; to scream or cry violently, as a w...     none  \n",
       "1     to cry out; to scream or cry violently, as a w...     none  \n",
       "2     to cry out; to scream or cry violently, as a w...  related  \n",
       "3                      swelled, enlarged, or distended;  related  \n",
       "4                  rising above the level; protuberant.     none  \n",
       "...                                                 ...      ...  \n",
       "8332  to make an attempt; to make an essay or a tria...     none  \n",
       "8333  to present in words; to proffer; to make a pro...     none  \n",
       "8334                          to attempt; to undertake.     none  \n",
       "8335              to bid, as a price, reward, or wages;     none  \n",
       "8336  to put in opposition to; to manifest in an off...     none  \n",
       "\n",
       "[8337 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('english_nuig_binary.tsv',sep='\\t', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/seungbinyim/Development/repos/codalab/Codalab')\n",
    "from classifier_config import ClassifierConfig\n",
    "from feature_extractor import FeatureExtractor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from model_trainer import ModelTrainer\n",
    "from wsa_classifier import WordSenseAlignmentClassifier\n",
    "from copy import deepcopy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "from nltk.corpus import wordnet as wn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_config = ClassifierConfig('en_core_web_lg', \"english\", 'data/train', balancing_strategy=\"none\",\n",
    "                                      testset_ratio=0.0, with_wordnet= True, dataset='english_nuig', logger = 'en_nuig')\n",
    "feature_extractor = FeatureExtractor() \\\n",
    "        .diff_pos_count() \\\n",
    "        .ont_hot_pos() \\\n",
    "        .matching_lemma() \\\n",
    "        .count_each_pos() \\\n",
    "        .avg_count_synsets() \\\n",
    "        .difference_in_length()\\\n",
    "        .similarity_diff_to_target()\\\n",
    "        .max_dependency_tree_depth() \\\n",
    "        .target_word_synset_count()\\\n",
    "        .token_count_norm_diff()\\\n",
    "        .semicol_count()\\\n",
    "    #.elmo_similarity()\n",
    "\n",
    "rf = {\n",
    "    'estimator': RandomForestClassifier(),\n",
    "    'parameters': {\n",
    "        'class_weight': ['balanced', 'balanced_subsample', ],\n",
    "        'max_depth': [10, 20],\n",
    "        'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "        'min_samples_leaf': [2],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'n_estimators': [300, 800],\n",
    "        'n_jobs':[8]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_trainer = ModelTrainer(english_config.testset_ratio, english_config.logger)\n",
    "model_trainer.add_estimators([rf])\n",
    "english_classifier = WordSenseAlignmentClassifier(english_config, feature_extractor, model_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = english_classifier.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_binary = deepcopy(data._data)\n",
    "data_binary['relation'] = data_binary['relation'].map(lambda rel: related(rel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = feature_extractor.extract(data_binary, ['len_diff','pos_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'class_weight': ['balanced', 'balanced_subsample'],\n",
      " 'max_depth': [10, 20],\n",
      " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
      " 'min_samples_leaf': [2],\n",
      " 'min_samples_split': [5, 10],\n",
      " 'n_estimators': [300, 800],\n",
      " 'n_jobs': [8]}\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'class_weight': ['balanced', 'balanced_subsample'],\n",
      " 'max_depth': [10, 20],\n",
      " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
      " 'min_samples_leaf': [2],\n",
      " 'min_samples_split': [5, 10],\n",
      " 'n_estimators': [300, 800],\n",
      " 'n_jobs': [8]}\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'class_weight': ['balanced', 'balanced_subsample'],\n",
      " 'max_depth': [10, 20],\n",
      " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
      " 'min_samples_leaf': [2],\n",
      " 'min_samples_split': [5, 10],\n",
      " 'n_estimators': [300, 800],\n",
      " 'n_jobs': [8]}\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  7.5min finished\n"
     ]
    }
   ],
   "source": [
    "models = model_trainer.train(feats, data_binary['relation'],with_testset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                        criterion='gini', max_depth=10, max_features='log2',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=2, min_samples_split=10,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=8,\n",
       "                        oob_score=False, random_state=None, verbose=0,\n",
       "                        warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                        criterion='gini', max_depth=20, max_features='sqrt',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=2, min_samples_split=5,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=8,\n",
       "                        oob_score=False, random_state=None, verbose=0,\n",
       "                        warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                        criterion='gini', max_depth=20, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=2, min_samples_split=5,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=8,\n",
       "                        oob_score=False, random_state=None, verbose=0,\n",
       "                        warm_start=False),\n",
       " VotingClassifier(estimators=[('RandomForestClassifier04fbb6eb-998f-4421-8fe0-e5e1a532bc76',\n",
       "                               RandomForestClassifier(bootstrap=True,\n",
       "                                                      ccp_alpha=0.0,\n",
       "                                                      class_weight='balanced',\n",
       "                                                      criterion='gini',\n",
       "                                                      max_depth=10,\n",
       "                                                      max_features='log2',\n",
       "                                                      max_leaf_nodes=None,\n",
       "                                                      max_samples=None,\n",
       "                                                      min_impurity_decrease=0.0,\n",
       "                                                      min_impurity_split=None,\n",
       "                                                      min_samples_leaf=2,\n",
       "                                                      min_samples_split=10,\n",
       "                                                      min_we...\n",
       "                                                      criterion='gini',\n",
       "                                                      max_depth=20,\n",
       "                                                      max_features='auto',\n",
       "                                                      max_leaf_nodes=None,\n",
       "                                                      max_samples=None,\n",
       "                                                      min_impurity_decrease=0.0,\n",
       "                                                      min_impurity_split=None,\n",
       "                                                      min_samples_leaf=2,\n",
       "                                                      min_samples_split=5,\n",
       "                                                      min_weight_fraction_leaf=0.0,\n",
       "                                                      n_estimators=300, n_jobs=8,\n",
       "                                                      oob_score=False,\n",
       "                                                      random_state=None,\n",
       "                                                      verbose=0,\n",
       "                                                      warm_start=False))],\n",
       "                  flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                  weights=None)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = best_model.predict_proba(feats)\n",
    "probs_df = pd.DataFrame(probs)\n",
    "related_vec = probs_df[1]\n",
    "related_vec.name = 'relatedness'\n",
    "feats_related = feats.join(related_vec)\n",
    "feats_related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_word_same</th>\n",
       "      <th>similarities</th>\n",
       "      <th>pos_diff</th>\n",
       "      <th>cos_tfidf</th>\n",
       "      <th>verb</th>\n",
       "      <th>adverb</th>\n",
       "      <th>adjective</th>\n",
       "      <th>noun</th>\n",
       "      <th>lemma_match_normalized</th>\n",
       "      <th>AUX</th>\n",
       "      <th>...</th>\n",
       "      <th>len_diff</th>\n",
       "      <th>simdiff_to_target</th>\n",
       "      <th>max_depth_deptree_1</th>\n",
       "      <th>max_depth_deptree_2</th>\n",
       "      <th>target_word_synset_count</th>\n",
       "      <th>token_count_norm_diff</th>\n",
       "      <th>semicol_count1_norm</th>\n",
       "      <th>semicol_count2_norm</th>\n",
       "      <th>semicol_diff</th>\n",
       "      <th>relatedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.690449</td>\n",
       "      <td>-1.047162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.752110</td>\n",
       "      <td>0.690925</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.770526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.107833</td>\n",
       "      <td>-1.107833</td>\n",
       "      <td>0.052124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.717846</td>\n",
       "      <td>-0.689953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.752110</td>\n",
       "      <td>-0.042160</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.413021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.107833</td>\n",
       "      <td>-1.107833</td>\n",
       "      <td>0.082992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.788656</td>\n",
       "      <td>-1.047162</td>\n",
       "      <td>0.273604</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533028</td>\n",
       "      <td>0.013535</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.651358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.107833</td>\n",
       "      <td>-1.107833</td>\n",
       "      <td>0.827039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.684154</td>\n",
       "      <td>1.453301</td>\n",
       "      <td>0.296092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553917</td>\n",
       "      <td>-0.553917</td>\n",
       "      <td>0.780044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.600150</td>\n",
       "      <td>0.381674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553917</td>\n",
       "      <td>-0.553917</td>\n",
       "      <td>0.061014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>False</td>\n",
       "      <td>0.771537</td>\n",
       "      <td>-0.332744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.752110</td>\n",
       "      <td>-0.139813</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.406760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.107833</td>\n",
       "      <td>-1.107833</td>\n",
       "      <td>0.020648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8333</th>\n",
       "      <td>False</td>\n",
       "      <td>0.736976</td>\n",
       "      <td>-1.047162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.752110</td>\n",
       "      <td>-0.826633</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.498058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.323500</td>\n",
       "      <td>-3.323500</td>\n",
       "      <td>0.050839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>False</td>\n",
       "      <td>0.682163</td>\n",
       "      <td>1.096092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.752110</td>\n",
       "      <td>-0.205281</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0.164873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553917</td>\n",
       "      <td>-0.553917</td>\n",
       "      <td>0.035871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8335</th>\n",
       "      <td>False</td>\n",
       "      <td>0.671622</td>\n",
       "      <td>-0.332744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.752110</td>\n",
       "      <td>-0.324619</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.146927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553917</td>\n",
       "      <td>-0.553917</td>\n",
       "      <td>0.057794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8336</th>\n",
       "      <td>False</td>\n",
       "      <td>0.738669</td>\n",
       "      <td>-0.332744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.752110</td>\n",
       "      <td>-0.267468</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.354793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.661750</td>\n",
       "      <td>-1.661750</td>\n",
       "      <td>0.025224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8337 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      first_word_same  similarities  pos_diff  cos_tfidf  verb  adverb  \\\n",
       "0               False      0.690449 -1.047162   0.000000   1.0     0.0   \n",
       "1               False      0.717846 -0.689953   0.000000   1.0     0.0   \n",
       "2               False      0.788656 -1.047162   0.273604   1.0     0.0   \n",
       "3               False      0.684154  1.453301   0.296092   0.0     0.0   \n",
       "4               False      0.600150  0.381674   0.000000   0.0     0.0   \n",
       "...               ...           ...       ...        ...   ...     ...   \n",
       "8332            False      0.771537 -0.332744   0.000000   1.0     0.0   \n",
       "8333            False      0.736976 -1.047162   0.000000   1.0     0.0   \n",
       "8334            False      0.682163  1.096092   0.000000   1.0     0.0   \n",
       "8335            False      0.671622 -0.332744   0.000000   1.0     0.0   \n",
       "8336            False      0.738669 -0.332744   0.000000   1.0     0.0   \n",
       "\n",
       "      adjective  noun  lemma_match_normalized       AUX  ...  len_diff  \\\n",
       "0           0.0   0.0                0.000000  0.200626  ... -0.752110   \n",
       "1           0.0   0.0                0.000000  0.200626  ... -0.752110   \n",
       "2           0.0   0.0                0.076923  0.200626  ... -0.533028   \n",
       "3           1.0   0.0                0.125000  0.200626  ... -0.971192   \n",
       "4           1.0   0.0                0.000000  0.200626  ... -0.971192   \n",
       "...         ...   ...                     ...       ...  ...       ...   \n",
       "8332        0.0   0.0                0.000000  0.200626  ... -0.752110   \n",
       "8333        0.0   0.0                0.000000  0.200626  ... -0.752110   \n",
       "8334        0.0   0.0                0.000000  0.200626  ... -0.752110   \n",
       "8335        0.0   0.0                0.000000  0.200626  ... -0.752110   \n",
       "8336        0.0   0.0                0.000000  0.200626  ... -0.752110   \n",
       "\n",
       "      simdiff_to_target  max_depth_deptree_1  max_depth_deptree_2  \\\n",
       "0              0.690925                    3                    5   \n",
       "1             -0.042160                    3                    5   \n",
       "2              0.013535                    2                    5   \n",
       "3              0.000000                    3                    2   \n",
       "4              0.000000                    3                    4   \n",
       "...                 ...                  ...                  ...   \n",
       "8332          -0.139813                    2                    4   \n",
       "8333          -0.826633                    2                    4   \n",
       "8334          -0.205281                    2                    2   \n",
       "8335          -0.324619                    2                    4   \n",
       "8336          -0.267468                    2                    4   \n",
       "\n",
       "      target_word_synset_count  token_count_norm_diff  semicol_count1_norm  \\\n",
       "0                            3              -0.770526                  0.0   \n",
       "1                            3              -0.413021                  0.0   \n",
       "2                            3              -0.651358                  0.0   \n",
       "3                            3               0.470411                  0.0   \n",
       "4                            3               0.470411                  0.0   \n",
       "...                        ...                    ...                  ...   \n",
       "8332                        13              -0.406760                  0.0   \n",
       "8333                        13              -1.498058                  0.0   \n",
       "8334                        13               0.164873                  0.0   \n",
       "8335                        13              -0.146927                  0.0   \n",
       "8336                        13              -0.354793                  0.0   \n",
       "\n",
       "      semicol_count2_norm  semicol_diff  relatedness  \n",
       "0                1.107833     -1.107833     0.052124  \n",
       "1                1.107833     -1.107833     0.082992  \n",
       "2                1.107833     -1.107833     0.827039  \n",
       "3                0.553917     -0.553917     0.780044  \n",
       "4                0.553917     -0.553917     0.061014  \n",
       "...                   ...           ...          ...  \n",
       "8332             1.107833     -1.107833     0.020648  \n",
       "8333             3.323500     -3.323500     0.050839  \n",
       "8334             0.553917     -0.553917     0.035871  \n",
       "8335             0.553917     -0.553917     0.057794  \n",
       "8336             1.661750     -1.661750     0.025224  \n",
       "\n",
       "[8337 rows x 40 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'class_weight': ['balanced', 'balanced_subsample'],\n",
      " 'max_depth': [10, 20],\n",
      " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
      " 'min_samples_leaf': [2],\n",
      " 'min_samples_split': [5, 10],\n",
      " 'n_estimators': [300, 800],\n",
      " 'n_jobs': [8]}\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'class_weight': ['balanced', 'balanced_subsample'],\n",
      " 'max_depth': [10, 20],\n",
      " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
      " 'min_samples_leaf': [2],\n",
      " 'min_samples_split': [5, 10],\n",
      " 'n_estimators': [300, 800],\n",
      " 'n_jobs': [8]}\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'class_weight': ['balanced', 'balanced_subsample'],\n",
      " 'max_depth': [10, 20],\n",
      " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
      " 'min_samples_leaf': [2],\n",
      " 'min_samples_split': [5, 10],\n",
      " 'n_estimators': [300, 800],\n",
      " 'n_jobs': [8]}\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  6.5min finished\n"
     ]
    }
   ],
   "source": [
    "models_5class = model_trainer.train(feats_related, data._data['relation'],with_testset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                        class_weight='balanced_subsample', criterion='gini',\n",
       "                        max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=2,\n",
       "                        min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=300, n_jobs=8, oob_score=False,\n",
       "                        random_state=None, verbose=0, warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                        class_weight='balanced_subsample', criterion='gini',\n",
       "                        max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=2,\n",
       "                        min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=800, n_jobs=8, oob_score=False,\n",
       "                        random_state=None, verbose=0, warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                        class_weight='balanced_subsample', criterion='gini',\n",
       "                        max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=2,\n",
       "                        min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=300, n_jobs=8, oob_score=False,\n",
       "                        random_state=None, verbose=0, warm_start=False),\n",
       " VotingClassifier(estimators=[('RandomForestClassifier242e8f3a-6de3-47dd-92ce-2ea3f3198a02',\n",
       "                               RandomForestClassifier(bootstrap=True,\n",
       "                                                      ccp_alpha=0.0,\n",
       "                                                      class_weight='balanced_subsample',\n",
       "                                                      criterion='gini',\n",
       "                                                      max_depth=10,\n",
       "                                                      max_features='auto',\n",
       "                                                      max_leaf_nodes=None,\n",
       "                                                      max_samples=None,\n",
       "                                                      min_impurity_decrease=0.0,\n",
       "                                                      min_impurity_split=None,\n",
       "                                                      min_samples_leaf=2,\n",
       "                                                      min_samples_split...\n",
       "                                                      criterion='gini',\n",
       "                                                      max_depth=20,\n",
       "                                                      max_features='auto',\n",
       "                                                      max_leaf_nodes=None,\n",
       "                                                      max_samples=None,\n",
       "                                                      min_impurity_decrease=0.0,\n",
       "                                                      min_impurity_split=None,\n",
       "                                                      min_samples_leaf=2,\n",
       "                                                      min_samples_split=5,\n",
       "                                                      min_weight_fraction_leaf=0.0,\n",
       "                                                      n_estimators=300, n_jobs=8,\n",
       "                                                      oob_score=False,\n",
       "                                                      random_state=None,\n",
       "                                                      verbose=0,\n",
       "                                                      warm_start=False))],\n",
       "                  flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                  weights=None)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_5class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = ClassifierConfig('en_core_web_lg', \"english\", 'data/test', balancing_strategy=\"none\",\n",
    "                                      testset_ratio=0.0, with_wordnet= True, dataset='english_nuig', logger = 'en_nuig', is_testdata=True)\n",
    "test_feature_extractor = FeatureExtractor() \\\n",
    "        .diff_pos_count() \\\n",
    "        .ont_hot_pos() \\\n",
    "        .matching_lemma() \\\n",
    "        .count_each_pos() \\\n",
    "        .avg_count_synsets() \\\n",
    "        .difference_in_length()\\\n",
    "        .similarity_diff_to_target()\\\n",
    "        .max_dependency_tree_depth() \\\n",
    "        .target_word_synset_count()\\\n",
    "        .token_count_norm_diff()\\\n",
    "        .semicol_count()\\\n",
    "    #.elmo_similarity()\n",
    "rf = {\n",
    "    'estimator': RandomForestClassifier(),\n",
    "    'parameters': {\n",
    "        'class_weight': ['balanced', 'balanced_subsample'],\n",
    "        'max_depth': [10, 20],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'min_samples_leaf': [2],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'n_estimators': [300, 800],\n",
    "        'n_jobs':[8]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "test_model_trainer = ModelTrainer(test_config.testset_ratio, test_config.logger)\n",
    "test_model_trainer.add_estimators([rf])\n",
    "test_classifier = WordSenseAlignmentClassifier(test_config, test_feature_extractor, test_model_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata =test_classifier.load_data().get_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_feature_extractor.extract(testdata, feats_to_scale = ['len_diff', 'pos_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset, empty = test_model_trainer.split_data(test_features, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict with Binary classifier\n",
    "test_binary = best_model.predict_proba(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_binary_rel_vec = pd.DataFrame(test_binary)[1]\n",
    "test_binary_rel_vec\n",
    "test_binary_rel_vec.name = 'relatedness'\n",
    "test_binary_feat = testset.join(test_binary_rel_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_word_same</th>\n",
       "      <th>similarities</th>\n",
       "      <th>pos_diff</th>\n",
       "      <th>cos_tfidf</th>\n",
       "      <th>verb</th>\n",
       "      <th>adverb</th>\n",
       "      <th>adjective</th>\n",
       "      <th>noun</th>\n",
       "      <th>lemma_match_normalized</th>\n",
       "      <th>AUX</th>\n",
       "      <th>...</th>\n",
       "      <th>len_diff</th>\n",
       "      <th>simdiff_to_target</th>\n",
       "      <th>max_depth_deptree_1</th>\n",
       "      <th>max_depth_deptree_2</th>\n",
       "      <th>target_word_synset_count</th>\n",
       "      <th>token_count_norm_diff</th>\n",
       "      <th>semicol_count1_norm</th>\n",
       "      <th>semicol_count2_norm</th>\n",
       "      <th>semicol_diff</th>\n",
       "      <th>relatedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.641546</td>\n",
       "      <td>0.686678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221630</td>\n",
       "      <td>-0.246587</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>-1.827548</td>\n",
       "      <td>0.247555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.731661</td>\n",
       "      <td>0.686678</td>\n",
       "      <td>0.347267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.582008</td>\n",
       "      <td>-0.190916</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.349914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>-1.827548</td>\n",
       "      <td>0.683896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.751481</td>\n",
       "      <td>2.716855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.505194</td>\n",
       "      <td>...</td>\n",
       "      <td>1.336856</td>\n",
       "      <td>-0.051665</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.511811</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>4.354271</td>\n",
       "      <td>0.139167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.791688</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>1.123649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.882605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>-1.827548</td>\n",
       "      <td>0.409512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.808421</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998795</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>4.354271</td>\n",
       "      <td>0.404687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>False</td>\n",
       "      <td>0.827427</td>\n",
       "      <td>-1.681863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.425169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>-0.097543</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.499702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>-0.609183</td>\n",
       "      <td>0.043939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>False</td>\n",
       "      <td>0.798985</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>-0.211726</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.831656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>-0.609183</td>\n",
       "      <td>0.048368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>False</td>\n",
       "      <td>0.890995</td>\n",
       "      <td>-1.005137</td>\n",
       "      <td>0.113132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>-1.115048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>-0.663050</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.598908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>False</td>\n",
       "      <td>0.691457</td>\n",
       "      <td>-0.666774</td>\n",
       "      <td>0.131586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368801</td>\n",
       "      <td>-0.170843</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.890169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>-0.609183</td>\n",
       "      <td>0.458067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>False</td>\n",
       "      <td>0.903305</td>\n",
       "      <td>-0.328411</td>\n",
       "      <td>0.422454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>2.402892</td>\n",
       "      <td>-0.141149</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.620296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>-0.609183</td>\n",
       "      <td>0.724011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>544 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     first_word_same  similarities  pos_diff  cos_tfidf  verb  adverb  \\\n",
       "0               True      0.641546  0.686678   0.000000   0.0     0.0   \n",
       "1               True      0.731661  0.686678   0.347267   0.0     0.0   \n",
       "2              False      0.751481  2.716855   0.000000   0.0     0.0   \n",
       "3              False      0.791688  0.348315   0.000000   0.0     0.0   \n",
       "4              False      0.808421  0.348315   0.000000   0.0     0.0   \n",
       "..               ...           ...       ...        ...   ...     ...   \n",
       "539            False      0.827427 -1.681863   0.000000   1.0     0.0   \n",
       "540            False      0.798985  0.348315   0.000000   1.0     0.0   \n",
       "541            False      0.890995 -1.005137   0.113132   1.0     0.0   \n",
       "542            False      0.691457 -0.666774   0.131586   0.0     0.0   \n",
       "543            False      0.903305 -0.328411   0.422454   0.0     0.0   \n",
       "\n",
       "     adjective  noun  lemma_match_normalized       AUX  ...  len_diff  \\\n",
       "0          1.0   0.0                0.000000  0.195073  ... -1.221630   \n",
       "1          1.0   0.0                0.166667  0.195073  ... -0.582008   \n",
       "2          1.0   0.0                0.000000  1.505194  ...  1.336856   \n",
       "3          0.0   1.0                0.000000  0.195073  ...  1.123649   \n",
       "4          0.0   1.0                0.000000  0.195073  ...  0.697234   \n",
       "..         ...   ...                     ...       ...  ...       ...   \n",
       "539        0.0   0.0                0.000000 -2.425169  ...  0.697234   \n",
       "540        0.0   0.0                0.000000  0.195073  ...  0.697234   \n",
       "541        0.0   0.0                0.038462 -1.115048  ...  0.697234   \n",
       "542        0.0   1.0                0.050000  0.195073  ... -0.368801   \n",
       "543        0.0   1.0                0.200000  0.195073  ...  2.402892   \n",
       "\n",
       "     simdiff_to_target  max_depth_deptree_1  max_depth_deptree_2  \\\n",
       "0            -0.246587                    1                    3   \n",
       "1            -0.190916                    2                    3   \n",
       "2            -0.051665                    7                    3   \n",
       "3             0.000000                    5                    3   \n",
       "4             0.000000                    6                    3   \n",
       "..                 ...                  ...                  ...   \n",
       "539          -0.097543                    5                    8   \n",
       "540          -0.211726                    5                    3   \n",
       "541          -0.663050                    5                   10   \n",
       "542          -0.170843                    3                    5   \n",
       "543          -0.141149                    6                    5   \n",
       "\n",
       "     target_word_synset_count  token_count_norm_diff  semicol_count1_norm  \\\n",
       "0                           3               0.001345             0.000000   \n",
       "1                           3               0.349914             0.000000   \n",
       "2                           3               1.511811             6.181818   \n",
       "3                           2               0.882605             0.000000   \n",
       "4                           2               0.998795             6.181818   \n",
       "..                        ...                    ...                  ...   \n",
       "539                        10              -1.499702             0.000000   \n",
       "540                        10               0.831656             0.000000   \n",
       "541                        10              -1.598908             0.000000   \n",
       "542                         2              -0.890169             0.000000   \n",
       "543                         2               0.620296             0.000000   \n",
       "\n",
       "     semicol_count2_norm  semicol_diff  relatedness  \n",
       "0               1.827548     -1.827548     0.247555  \n",
       "1               1.827548     -1.827548     0.683896  \n",
       "2               1.827548      4.354271     0.139167  \n",
       "3               1.827548     -1.827548     0.409512  \n",
       "4               1.827548      4.354271     0.404687  \n",
       "..                   ...           ...          ...  \n",
       "539             0.609183     -0.609183     0.043939  \n",
       "540             0.609183     -0.609183     0.048368  \n",
       "541             0.000000      0.000000     0.225948  \n",
       "542             0.609183     -0.609183     0.458067  \n",
       "543             0.609183     -0.609183     0.724011  \n",
       "\n",
       "[544 rows x 40 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_binary_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_word_same</th>\n",
       "      <th>similarities</th>\n",
       "      <th>pos_diff</th>\n",
       "      <th>cos_tfidf</th>\n",
       "      <th>verb</th>\n",
       "      <th>adverb</th>\n",
       "      <th>adjective</th>\n",
       "      <th>noun</th>\n",
       "      <th>lemma_match_normalized</th>\n",
       "      <th>AUX</th>\n",
       "      <th>...</th>\n",
       "      <th>len_diff</th>\n",
       "      <th>simdiff_to_target</th>\n",
       "      <th>max_depth_deptree_1</th>\n",
       "      <th>max_depth_deptree_2</th>\n",
       "      <th>target_word_synset_count</th>\n",
       "      <th>token_count_norm_diff</th>\n",
       "      <th>semicol_count1_norm</th>\n",
       "      <th>semicol_count2_norm</th>\n",
       "      <th>semicol_diff</th>\n",
       "      <th>relatedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.641546</td>\n",
       "      <td>0.686678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.221630</td>\n",
       "      <td>-0.246587</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>-1.827548</td>\n",
       "      <td>0.247555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.731661</td>\n",
       "      <td>0.686678</td>\n",
       "      <td>0.347267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.582008</td>\n",
       "      <td>-0.190916</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.349914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>-1.827548</td>\n",
       "      <td>0.683896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.751481</td>\n",
       "      <td>2.716855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.505194</td>\n",
       "      <td>...</td>\n",
       "      <td>1.336856</td>\n",
       "      <td>-0.051665</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.511811</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>4.354271</td>\n",
       "      <td>0.139167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.791688</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>1.123649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.882605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>-1.827548</td>\n",
       "      <td>0.409512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.808421</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998795</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>1.827548</td>\n",
       "      <td>4.354271</td>\n",
       "      <td>0.404687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>False</td>\n",
       "      <td>0.827427</td>\n",
       "      <td>-1.681863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.425169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>-0.097543</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.499702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>-0.609183</td>\n",
       "      <td>0.043939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>False</td>\n",
       "      <td>0.798985</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>-0.211726</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.831656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>-0.609183</td>\n",
       "      <td>0.048368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>False</td>\n",
       "      <td>0.890995</td>\n",
       "      <td>-1.005137</td>\n",
       "      <td>0.113132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>-1.115048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>-0.663050</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.598908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>False</td>\n",
       "      <td>0.691457</td>\n",
       "      <td>-0.666774</td>\n",
       "      <td>0.131586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368801</td>\n",
       "      <td>-0.170843</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.890169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>-0.609183</td>\n",
       "      <td>0.458067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>False</td>\n",
       "      <td>0.903305</td>\n",
       "      <td>-0.328411</td>\n",
       "      <td>0.422454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>...</td>\n",
       "      <td>2.402892</td>\n",
       "      <td>-0.141149</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.620296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>-0.609183</td>\n",
       "      <td>0.724011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>544 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     first_word_same  similarities  pos_diff  cos_tfidf  verb  adverb  \\\n",
       "0               True      0.641546  0.686678   0.000000   0.0     0.0   \n",
       "1               True      0.731661  0.686678   0.347267   0.0     0.0   \n",
       "2              False      0.751481  2.716855   0.000000   0.0     0.0   \n",
       "3              False      0.791688  0.348315   0.000000   0.0     0.0   \n",
       "4              False      0.808421  0.348315   0.000000   0.0     0.0   \n",
       "..               ...           ...       ...        ...   ...     ...   \n",
       "539            False      0.827427 -1.681863   0.000000   1.0     0.0   \n",
       "540            False      0.798985  0.348315   0.000000   1.0     0.0   \n",
       "541            False      0.890995 -1.005137   0.113132   1.0     0.0   \n",
       "542            False      0.691457 -0.666774   0.131586   0.0     0.0   \n",
       "543            False      0.903305 -0.328411   0.422454   0.0     0.0   \n",
       "\n",
       "     adjective  noun  lemma_match_normalized       AUX  ...  len_diff  \\\n",
       "0          1.0   0.0                0.000000  0.195073  ... -1.221630   \n",
       "1          1.0   0.0                0.166667  0.195073  ... -0.582008   \n",
       "2          1.0   0.0                0.000000  1.505194  ...  1.336856   \n",
       "3          0.0   1.0                0.000000  0.195073  ...  1.123649   \n",
       "4          0.0   1.0                0.000000  0.195073  ...  0.697234   \n",
       "..         ...   ...                     ...       ...  ...       ...   \n",
       "539        0.0   0.0                0.000000 -2.425169  ...  0.697234   \n",
       "540        0.0   0.0                0.000000  0.195073  ...  0.697234   \n",
       "541        0.0   0.0                0.038462 -1.115048  ...  0.697234   \n",
       "542        0.0   1.0                0.050000  0.195073  ... -0.368801   \n",
       "543        0.0   1.0                0.200000  0.195073  ...  2.402892   \n",
       "\n",
       "     simdiff_to_target  max_depth_deptree_1  max_depth_deptree_2  \\\n",
       "0            -0.246587                    1                    3   \n",
       "1            -0.190916                    2                    3   \n",
       "2            -0.051665                    7                    3   \n",
       "3             0.000000                    5                    3   \n",
       "4             0.000000                    6                    3   \n",
       "..                 ...                  ...                  ...   \n",
       "539          -0.097543                    5                    8   \n",
       "540          -0.211726                    5                    3   \n",
       "541          -0.663050                    5                   10   \n",
       "542          -0.170843                    3                    5   \n",
       "543          -0.141149                    6                    5   \n",
       "\n",
       "     target_word_synset_count  token_count_norm_diff  semicol_count1_norm  \\\n",
       "0                           3               0.001345             0.000000   \n",
       "1                           3               0.349914             0.000000   \n",
       "2                           3               1.511811             6.181818   \n",
       "3                           2               0.882605             0.000000   \n",
       "4                           2               0.998795             6.181818   \n",
       "..                        ...                    ...                  ...   \n",
       "539                        10              -1.499702             0.000000   \n",
       "540                        10               0.831656             0.000000   \n",
       "541                        10              -1.598908             0.000000   \n",
       "542                         2              -0.890169             0.000000   \n",
       "543                         2               0.620296             0.000000   \n",
       "\n",
       "     semicol_count2_norm  semicol_diff  relatedness  \n",
       "0               1.827548     -1.827548     0.247555  \n",
       "1               1.827548     -1.827548     0.683896  \n",
       "2               1.827548      4.354271     0.139167  \n",
       "3               1.827548     -1.827548     0.409512  \n",
       "4               1.827548      4.354271     0.404687  \n",
       "..                   ...           ...          ...  \n",
       "539             0.609183     -0.609183     0.043939  \n",
       "540             0.609183     -0.609183     0.048368  \n",
       "541             0.000000      0.000000     0.225948  \n",
       "542             0.609183     -0.609183     0.458067  \n",
       "543             0.609183     -0.609183     0.724011  \n",
       "\n",
       "[544 rows x 40 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_binary_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict with 5 class\n",
    "five_class_result = models_5class[2].predict(test_binary_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_series = pd.Series(five_class_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata['relation'] = result_series\n",
    "five_class_predcted = testdata[['word','pos','def1','def2','relation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_class_predcted.to_csv('doublelayer.tsv',sep='\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>def1</th>\n",
       "      <th>def2</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unusual</td>\n",
       "      <td>adjective</td>\n",
       "      <td>not commonly encountered</td>\n",
       "      <td>not usual; uncommon; rare;</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unusual</td>\n",
       "      <td>adjective</td>\n",
       "      <td>not usual or common or ordinary</td>\n",
       "      <td>not usual; uncommon; rare;</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unusual</td>\n",
       "      <td>adjective</td>\n",
       "      <td>being definitely out of the ordinary and unexp...</td>\n",
       "      <td>not usual; uncommon; rare;</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tramper</td>\n",
       "      <td>noun</td>\n",
       "      <td>someone who walks with a heavy noisy gait or w...</td>\n",
       "      <td>one who tramps; a stroller; a vagrant or vagab...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tramper</td>\n",
       "      <td>noun</td>\n",
       "      <td>a foot traveler; someone who goes on an extend...</td>\n",
       "      <td>one who tramps; a stroller; a vagrant or vagab...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>register</td>\n",
       "      <td>verb</td>\n",
       "      <td>record in a public office or in a court of law</td>\n",
       "      <td>to correspond in relative position;  ,  when t...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>register</td>\n",
       "      <td>verb</td>\n",
       "      <td>record in a public office or in a court of law</td>\n",
       "      <td>to enroll; to enter in a list.</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>register</td>\n",
       "      <td>verb</td>\n",
       "      <td>record in a public office or in a court of law</td>\n",
       "      <td>to enter the name of the owner of (a share of ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>cottonwood</td>\n",
       "      <td>noun</td>\n",
       "      <td>American basswood of the Allegheny region</td>\n",
       "      <td>an american tree of the genus  or poplar, havi...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>cottonwood</td>\n",
       "      <td>noun</td>\n",
       "      <td>any of several North American trees of the gen...</td>\n",
       "      <td>an american tree of the genus  or poplar, havi...</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>544 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word        pos                                               def1  \\\n",
       "0       unusual  adjective                           not commonly encountered   \n",
       "1       unusual  adjective                    not usual or common or ordinary   \n",
       "2       unusual  adjective  being definitely out of the ordinary and unexp...   \n",
       "3       tramper       noun  someone who walks with a heavy noisy gait or w...   \n",
       "4       tramper       noun  a foot traveler; someone who goes on an extend...   \n",
       "..          ...        ...                                                ...   \n",
       "539    register       verb     record in a public office or in a court of law   \n",
       "540    register       verb     record in a public office or in a court of law   \n",
       "541    register       verb     record in a public office or in a court of law   \n",
       "542  cottonwood       noun          American basswood of the Allegheny region   \n",
       "543  cottonwood       noun  any of several North American trees of the gen...   \n",
       "\n",
       "                                                  def2 relation  \n",
       "0                           not usual; uncommon; rare;     none  \n",
       "1                           not usual; uncommon; rare;    exact  \n",
       "2                           not usual; uncommon; rare;     none  \n",
       "3    one who tramps; a stroller; a vagrant or vagab...     none  \n",
       "4    one who tramps; a stroller; a vagrant or vagab...     none  \n",
       "..                                                 ...      ...  \n",
       "539  to correspond in relative position;  ,  when t...     none  \n",
       "540                     to enroll; to enter in a list.     none  \n",
       "541  to enter the name of the owner of (a share of ...     none  \n",
       "542  an american tree of the genus  or poplar, havi...     none  \n",
       "543  an american tree of the genus  or poplar, havi...    exact  \n",
       "\n",
       "[544 rows x 5 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_class_predcted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmo2",
   "language": "python",
   "name": "elmo2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
