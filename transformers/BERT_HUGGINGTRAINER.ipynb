{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837c85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 15 13:08:29 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   49C    P0    67W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   73C    P0    77W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   49C    P0    61W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla K80           Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   73C    P0    77W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla K80           Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   48C    P0    60W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla K80           Off  | 00000000:00:09.0 Off |                    0 |\n",
      "| N/A   73C    P0    76W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla K80           Off  | 00000000:00:0A.0 Off |                    0 |\n",
      "| N/A   48C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla K80           Off  | 00000000:00:0B.0 Off |                    0 |\n",
      "| N/A   74C    P0    95W / 149W |      0MiB / 11441MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "ln: failed to create symbolic link '/usr/bin/nvidia-smi': Permission denied\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: gputil in /opt/conda/lib/python3.7/site-packages (1.4.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (5.8.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: humanize in /opt/conda/lib/python3.7/site-packages (3.5.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from humanize) (49.6.0.post20210108)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Gen RAM Free: 14.7 GB  I Proc size: 185.5 MB\n",
      "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "## https://medium.com/@djajafer/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "## BERT for Multiclass\n",
    "## https://medium.com/@chandaksumit29_15695/bert-for-multi-class-text-classification-12b66a1fc01c\n",
    "\n",
    "# Dataset\n",
    "## Imbalance\n",
    "## https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a#:~:text=Multi%2Dclass%20classification%20makes%20the,classes%20are%20not%20represented%20equally.\n",
    "#https://stackoverflow.com/questions/61969783/huggingface-bert-showing-poor-accuracy-f1-score-pytorch\n",
    "#Comparatively Finetunign BERT https://github.com/uzaymacar/comparatively-finetuning-bert\n",
    "# Finetuning BERT for sentiment analysis\n",
    "## https://skimai.com/fine-tuning-bert-for-sentiment-analysis/\n",
    "\n",
    "# Training\n",
    "## Batch Normalization\n",
    "## https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
    "## Batchsize\n",
    "## https://dmitry.ai/t/topic/100\n",
    "\n",
    "# Loss Function\n",
    "## F1-Weighted Loss Function: https://stackoverflow.com/questions/59sam63911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n",
    "## How to choose loss function: https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "## Loss function for f1 https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "## Cross Entropy: https://stats.stackexchange.com/questions/272754/how-do-you-interpret-the-cross-entropy-value\n",
    "## https://stats.stackexchange.com/questions/207794/what-loss-function-for-multi-class-multi-label-classification-tasks-in-neural-n\n",
    "\n",
    "# Problens while finetuning\n",
    "# https://github.com/google-research/bert/issues/663\n",
    "## High Loss Good accuracy\n",
    "## https://stats.stackexchange.com/questions/258166/good-accuracy-despite-high-loss-value/281651#281651\n",
    "## High Accuracy Low F1\n",
    "## https://www.quora.com/How-do-I-interpret-a-high-accuracy-82-44-but-an-extremely-low-F1-score-0-259-in-my-deep-learning-model-How-do-I-go-about-improving-the-F1-score\n",
    "## How is it possible that validation loss is increasing while validation accuracy is increasing as well\n",
    "## https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize    \n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn't guaranteed\n",
    "gpu = GPUs[0]\n",
    "#gpu2 = GPUs[1]\n",
    "\n",
    "def printm():\n",
    "  process = psutil.Process(os.getpid())\n",
    "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "  print('GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB'.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "#  print('GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB'.format(gpu2.memoryFree, gpu2.memoryUsed, gpu2.memoryUtil*100, gpu2.memoryTotal))\n",
    "    \n",
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f873623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo kill -9 1748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a2643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo fuser -v /dev/nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541e2dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.5.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.6.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.49.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.0.1)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.0.8)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: focal_loss in /opt/conda/lib/python3.7/site-packages (0.0.6)\n",
      "Requirement already satisfied: tensorflow>=2.2 in /opt/conda/lib/python3.7/site-packages (from focal_loss) (2.4.1)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.15.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.10.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.32.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.15.8)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.2.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.4.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.36.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.3.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.10.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.19.5)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.12)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.30.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (49.6.0.post20210108)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (0.6.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (4.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (3.4.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "!pip install focal_loss\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:\n",
    "    import os\n",
    "\n",
    "    \n",
    "    os.kill(os.getpid(), 9)\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets import ClassLabel, Features, Value\n",
    "# Tensorflow classification with imbalanced data https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#calculate_class_weights\n",
    "# How to choose loss function https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "#https://colab.research.google.com/drive/1kEg0SnYNtw_IJwu_kl5y3qRVs-BKBmNO#scrollTo=W64yQeCyl8sC\n",
    "#https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb\n",
    "#https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb\n",
    "#https://huggingface.co/transformers/master/custom_datasets.html\n",
    "#https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f732d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=256\n",
    "model_name='roberta-large'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd067d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(file_path):\n",
    "    loaded_data = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    return loaded_data\n",
    "\n",
    "\n",
    "def reverse_multihot(val):\n",
    "    encoding = {0:'none', 1:'exact', 2:'broader', 3:'narrower', 4:'related'}\n",
    "    return encoding[val]\n",
    "\n",
    "def split_data(data, testset_ratio):\n",
    "    train_df, remaining = train_test_split(data, random_state=42, train_size=0.75, stratify=data.labels.values)\n",
    "    return train_df, remaining\n",
    "    #data = data.sample(frac=1)\n",
    "    #f = int(len(data) * testset_ratio)\n",
    "    #return data[f:], data[:f]\n",
    "\n",
    "def multihot(val):\n",
    "    encoding = {'none':0, 'exact':1, 'broader':2, 'narrower':3, 'related':4}\n",
    "    return encoding[val]\n",
    "\n",
    "\n",
    "def encode(data):\n",
    "  return tokenizer(data['text_a'], data['text_b'], truncation=True, max_length=max_length, padding='max_length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91288bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none        7137\n",
       "exact        800\n",
       "narrower     310\n",
       "related       51\n",
       "broader       39\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "folder = 'english_nuig.tsv'\n",
    "en_data = load_data(folder)\n",
    "en_data = en_data[[2,3,4]]\n",
    "en_data.columns =  ['text_a','text_b','labels']\n",
    "en_data['labels'].value_counts()\n",
    "#en_data['labels'] = en_data['labels'].map(lambda val: multihot(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d41fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'english_nuig_reference.tsv'\n",
    "test_data_orig = load_data(test_file)\n",
    "test_data = test_data_orig[[2,3,4]]\n",
    "test_data.columns = ['text_a', 'text_b', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e70aa8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3\n",
       "1      2\n",
       "2      3\n",
       "3      4\n",
       "4      3\n",
       "      ..\n",
       "539    4\n",
       "540    4\n",
       "541    3\n",
       "542    4\n",
       "543    2\n",
       "Name: labels, Length: 544, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ['broader','narrower','exact','related','none']\n",
    "en_data['labels'].apply(label_list.index)\n",
    "test_data['labels'].apply(label_list.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29add1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rouse somebody from sleep with a call',\n",
       "       \"insist on having one's opinions and rights recognized\",\n",
       "       'give a workout to', ...,\n",
       "       'a layer of material that encloses space', 'cause annoyance in',\n",
       "       'loss resulting from failure of a debt to be paid'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset, valset = split_data(en_data, 0.2)\n",
    "trainset.shape, valset.shape\n",
    "trainset.text_a.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa735aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b140e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e2f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae8ed137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'broader', 1: 'exact', 2: 'narrower', 3: 'none', 4: 'related'}\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(en_data['labels'].to_list())\n",
    "\n",
    "labels = le.classes_\n",
    "labelMap = {v: k for v, k in enumerate(labels)}\n",
    "    \n",
    "print(labelMap)\n",
    "\n",
    "def onehot_encode(data, encoder):\n",
    "\n",
    "  integer_encoded = encoder.transform(data['labels'].to_list())\n",
    "  onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "  \n",
    "  return onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2aa2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded_train = onehot_encode(trainset, le)\n",
    "onehot_encoded_val = onehot_encode(valset, le)\n",
    "onehot_encoded_test = onehot_encode(test_data, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9d7d867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>rouse somebody from sleep with a call</td>\n",
       "      <td>to regard or characterize as of a certain kind...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>insist on having one's opinions and rights rec...</td>\n",
       "      <td>to maintain or defend, as a cause or a claim, ...</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>give a workout to</td>\n",
       "      <td>to exert one's self for a purpose; to put fort...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>belonging to or characteristic of the nobility...</td>\n",
       "      <td>having the color of the clear sky, or a hue re...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>insert or adjust several objects or people</td>\n",
       "      <td>to be proper or becoming.</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5587</th>\n",
       "      <td>the sound made by a gentle blow</td>\n",
       "      <td>a light, quik blow or stroke with the fingers ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7899</th>\n",
       "      <td>teach or refine to be discriminative in taste ...</td>\n",
       "      <td>to draw by persuasion, artifice, or the like; ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>a layer of material that encloses space</td>\n",
       "      <td>a kind of knot often used at the end of a rope...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>cause annoyance in</td>\n",
       "      <td>to be agitated; to be in violent commotion; to...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>loss resulting from failure of a debt to be paid</td>\n",
       "      <td>a failing or failure; omission of that which o...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6252 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text_a  \\\n",
       "2199              rouse somebody from sleep with a call   \n",
       "3779  insist on having one's opinions and rights rec...   \n",
       "3909                                  give a workout to   \n",
       "1881  belonging to or characteristic of the nobility...   \n",
       "3466         insert or adjust several objects or people   \n",
       "...                                                 ...   \n",
       "5587                    the sound made by a gentle blow   \n",
       "7899  teach or refine to be discriminative in taste ...   \n",
       "2665            a layer of material that encloses space   \n",
       "633                                  cause annoyance in   \n",
       "2695   loss resulting from failure of a debt to be paid   \n",
       "\n",
       "                                                 text_b labels  \n",
       "2199  to regard or characterize as of a certain kind...   none  \n",
       "3779  to maintain or defend, as a cause or a claim, ...  exact  \n",
       "3909  to exert one's self for a purpose; to put fort...   none  \n",
       "1881  having the color of the clear sky, or a hue re...   none  \n",
       "3466                          to be proper or becoming.   none  \n",
       "...                                                 ...    ...  \n",
       "5587  a light, quik blow or stroke with the fingers ...   none  \n",
       "7899  to draw by persuasion, artifice, or the like; ...   none  \n",
       "2665  a kind of knot often used at the end of a rope...   none  \n",
       "633   to be agitated; to be in violent commotion; to...   none  \n",
       "2695  a failing or failure; omission of that which o...   none  \n",
       "\n",
       "[6252 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110321b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoded_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26af4b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['none'], dtype='<U8')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y = le.inverse_transform([numpy.argmax(onehot_encoded_val[2, :])])\n",
    "val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e666d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1 2 3 4], y=[3 1 3 ... 3 3 3] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "y_integers = np.argmax(onehot_encoded_train, axis=1)\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "sample_weights = compute_sample_weight('balanced', onehot_encoded_train)\n",
    "sample_weights_val = compute_sample_weight('balanced', onehot_encoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a43d9e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.04239661830954378,\n",
       " 2.3749568058166997,\n",
       " 6.512882377104106,\n",
       " 41.22801874886851,\n",
       " 54.10116480927345}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f64d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = trainset['text_a'].to_list()\n",
    "train_b = trainset['text_b'].to_list()\n",
    "train_labels = onehot_encoded_train\n",
    "\n",
    "val_a = valset['text_a'].to_list()\n",
    "val_b = valset['text_b'].to_list()\n",
    "val_labels = onehot_encoded_val\n",
    "\n",
    "test_a = test_data['text_a'].to_list()\n",
    "test_b = test_data['text_b'].to_list()\n",
    "test_labels = onehot_encoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa3bb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270869dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ec2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_a, train_b, truncation=True, padding='longest',max_length=max_length, return_tensors='tf')\n",
    "val_encodings = tokenizer(val_a, val_b, truncation=True, padding='longest', max_length=max_length, return_tensors='tf')\n",
    "test_encodings = tokenizer(test_a, test_b, truncation=True,padding='longest', max_length=max_length,return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d8334dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels,\n",
    "    sample_weights\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels,\n",
    "    sample_weights_val\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))\n",
    "# Oversampling\n",
    "# https://github.com/tensorflow/tensorflow/issues/14451\n",
    "#https://keras.io/api/models/model_training_apis/  \n",
    "# https://stackoverflow.com/questions/53653485/training-with-tf-data-api-and-sample-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63493647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': TensorSpec(shape=(256,), dtype=tf.int32, name=None),\n",
       "  'attention_mask': TensorSpec(shape=(256,), dtype=tf.int32, name=None)},\n",
       " TensorSpec(shape=(5,), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.element_spec\n",
    "#!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06a95292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_datasets as tfds\n",
    "#for pair, label in tfds.as_numpy(train_dataset.take(1)):\n",
    "#    print('pair', pair, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f92d63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.10.29)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.15)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.15.8)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.0 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    }
   ],
   "source": [
    "train_labels[0]\n",
    "le.inverse_transform([0])\n",
    "!pip install wandb\n",
    "!wandb login 8d2052d6ffc37b0b4398c538a9604fe6b18e0cf9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23f8bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: focal_loss in /opt/conda/lib/python3.7/site-packages (0.0.6)\n",
      "Requirement already satisfied: tensorflow>=2.2 in /opt/conda/lib/python3.7/site-packages (from focal_loss) (2.4.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.5.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.19.5)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.15.8)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.3.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.10.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.12)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.36.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.32.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.10.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.2.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.4.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.12.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.30.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (0.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2->focal_loss) (2.25.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (4.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (1.26.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2->focal_loss) (3.4.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aba15386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 12.6 GB  I Proc size: 2.2 GB\n",
      "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
     ]
    }
   ],
   "source": [
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0012aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: tensorflow-addons==0.8.3 in /opt/conda/lib/python3.7/site-packages (0.8.3)\n",
      "Requirement already satisfied: typeguard in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons==0.8.3) (2.12.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install Keras==2.4.3\n",
    "!pip install tensorflow-addons==0.8.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0dc6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "import tensorflow_addons as tfa\n",
    "#from tfa.losses import SigmoidFocalCrossEntropy\n",
    "#https://www.tensorflow.org/guide/keras/custom_callback#an_overview_of_callback_methods\n",
    "class LossAndErrorPrintingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \".format(\n",
    "                epoch, logs[\"loss\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "class LRLogger(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LRLogger, self).__init__()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        #lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        lr = self.model.optimizer.learning_rate(self.model.optimizer.iterations)\n",
    "        print('learning rate: '+str(lr) + 'iteration: ' + str(self.model.optimizer.iterations) )\n",
    "        wandb.log({'epoch': epoch, 'learning_rate': lr})\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        #scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        #tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        #print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6f1cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU.getGPUs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ea73799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "#from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import TFRobertaForSequenceClassification,AdamWeightDecay\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "seed=777\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "#wandb.config.globalseed=seed\n",
    "#wandb.config.nr_gpus=len(GPU.getGPUs())\n",
    "#wandb.config.lr = 4e-5\n",
    "#wandb.config.weight_decay_rate=0.2\n",
    "#wandb.config.epoch=10\n",
    "#wandb.config.batch=32\n",
    "#wandb.config.max_length=max_length\n",
    "#wandb.config.lr_scheduler='ReduceLROnPlateau'\n",
    "#wandb.config.seed=seed\n",
    "#wandb.config.model=model_name\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06d51e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0)\n",
    "\n",
    "# Define multi-gpu strategy \n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "# Update batch size value\n",
    "#wandb.config.batch *= mirrored_strategy.num_replicas_in_sync\n",
    "# Create strategy scope to perform training\n",
    "\n",
    "# Transformer and Batchsize\n",
    "#https://dmitry.ai/t/topic/100/2\n",
    "\n",
    "\n",
    "class RemoveGarbaseCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "568a5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_dict={}\n",
    "for i in range(len(class_weights)):\n",
    "    class_weights_dict[i]=class_weights[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7cf3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization_tf import WarmUp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10b849ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
    "from transformers.optimization_tf import create_optimizer\n",
    "\n",
    "def train():\n",
    "    \n",
    "    hyperparameter_defaults = dict(\n",
    "        nr_gpus=len(GPU.getGPUs()),\n",
    "        lr=5e-6,\n",
    "        model=model_name,\n",
    "        globalseed=seed,\n",
    "        weight_decay_rate=0.1,\n",
    "        epoch=10,\n",
    "        batch=16,\n",
    "        max_length=max_length,\n",
    "        seed=seed\n",
    "    )\n",
    "    wandb.init(project=\"mwsa_transformers\", config=hyperparameter_defaults)\n",
    "    config = wandb.config\n",
    "    \n",
    "\n",
    "    #https://www.tensorflow.org/official_models/fine_tuning_bert\n",
    "    train_data_size = len(train_labels)\n",
    "    steps_per_epoch = int(train_data_size/config.batch)\n",
    "    num_train_steps = steps_per_epoch*config.epoch\n",
    "    warmup_steps = int(config.epoch*train_data_size*0.1/config.batch)\n",
    "    \n",
    "    min_lr_ratio: float = 0.0\n",
    "    power: float = 1.0\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=config.lr,\n",
    "        decay_steps=num_train_steps - warmup_steps,\n",
    "        end_learning_rate=config.lr * min_lr_ratio,\n",
    "        power=power,\n",
    "    )\n",
    "      \n",
    "    warmup_scheduler = WarmUp(config.lr, lr_schedule, warmup_steps=warmup_steps)\n",
    "    \n",
    "    with mirrored_strategy.scope():\n",
    "        #https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\n",
    "        #loss = {'issue': CategoricalCrossentropy(from_logits = False), 'product': CategoricalCrossentropy(from_logits = True)}\n",
    "        loss = SigmoidFocalCrossEntropy(from_logits=True, alpha=0.25, gamma=2.0)\n",
    "        metric = [CategoricalAccuracy(), F1Score(num_classes=5, average='weighted')]\n",
    "        # How to choose loss functions https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "        #https://stackoverflow.com/questions/59305514/tensorflow-how-to-use-tf-keras-metrics-in-multiclass-classification\n",
    "        #https://towardsdatascience.com/keras-accuracy-metrics-8572eb479ec7#:~:text=Categorical%20Accuracy%20calculates%20the%20percentage,yTrue%2C%20it%20is%20considered%20accurate.\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        #optimizer = AdamWeightDecay(learning_rate=0.01, weight_decay_rate=0.1)#silvery-plasma-24\n",
    "        optimizer = AdamWeightDecay(learning_rate=warmup_scheduler, weight_decay_rate=wandb.config.weight_decay_rate)\n",
    "        #optimizer = create_optimizer(init_lr=wandb.config.lr, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
    "        model = TFRobertaForSequenceClassification.from_pretrained(model_name, num_labels=5, return_dict=True)\n",
    "        #model.summary()\n",
    "        model.config.gradient_checkpointing=True\n",
    "\n",
    "        #https://keras.io/api/models/model_training_apis/\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metric, weighted_metrics=metric) # can also use any keras loss fn\n",
    "        history = model.fit(train_dataset.shuffle(1000,seed=wandb.config.seed).batch(wandb.config.batch), \n",
    "              epochs=wandb.config.epoch,\n",
    "              validation_data=val_dataset.shuffle(1000,seed=wandb.config.seed).batch(wandb.config.batch), \n",
    "                            class_weight=class_weights_dict,\n",
    "                            callbacks=[WandbCallback(),\n",
    "                                       reduce_lr,LRLogger(),\n",
    "                                       RemoveGarbaseCallback()])\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "# Steps per epoch\n",
    "# https://datascience.stackexchange.com/questions/47405/what-to-set-in-steps-per-epoch-in-keras-fit-generator\n",
    "\n",
    "#https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "# Increasing Loss and accuracy\n",
    "#Overfitting\n",
    "#https://datascience.stackexchange.com/questions/76527/overfitting-in-huggingfaces-tfbertforsequenceclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11135084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0700bb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: g44958cn\n",
      "Sweep URL: https://wandb.ai/sbyim/uncategorized/sweeps/g44958cn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msbyim\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.29<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">earthy-firebrand-308</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sbyim/mwsa_transformers\" target=\"_blank\">https://wandb.ai/sbyim/mwsa_transformers</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sbyim/mwsa_transformers/runs/177a97sc\" target=\"_blank\">https://wandb.ai/sbyim/mwsa_transformers/runs/177a97sc</a><br/>\n",
       "                Run data is saved locally in <code>/home/jupyter/wandb/run-20210515_131109-177a97sc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "learning rate: tf.Tensor(0.0, shape=(), dtype=float32)iteration: MirroredVariable:{\n",
      "  0: <tf.Variable 'iter:0' shape=() dtype=int64, numpy=0>,\n",
      "  1: <tf.Variable 'iter/replica_1:0' shape=() dtype=int64, numpy=0>,\n",
      "  2: <tf.Variable 'iter/replica_2:0' shape=() dtype=int64, numpy=0>,\n",
      "  3: <tf.Variable 'iter/replica_3:0' shape=() dtype=int64, numpy=0>,\n",
      "  4: <tf.Variable 'iter/replica_4:0' shape=() dtype=int64, numpy=0>,\n",
      "  5: <tf.Variable 'iter/replica_5:0' shape=() dtype=int64, numpy=0>,\n",
      "  6: <tf.Variable 'iter/replica_6:0' shape=() dtype=int64, numpy=0>,\n",
      "  7: <tf.Variable 'iter/replica_7:0' shape=() dtype=int64, numpy=0>\n",
      "}\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f83bf5ef980>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f83bf5ef980>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "INFO:tensorflow:batch_all_reduce: 390 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7').\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "INFO:tensorflow:batch_all_reduce: 390 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7').\n",
      "391/391 [==============================] - ETA: 0s - loss: 9.0432 - weighted_categorical_accuracy: 0.1932 - weighted_f1_score: 0.4472WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "391/391 [==============================] - 1429s 3s/step - loss: 9.0376 - weighted_categorical_accuracy: 0.1933 - weighted_f1_score: 0.4467 - val_loss: 0.3495 - val_weighted_categorical_accuracy: 0.1273 - val_weighted_f1_score: 4.5787e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "learning rate: tf.Tensor(4.998575e-06, shape=(), dtype=float32)iteration: MirroredVariable:{\n",
      "  0: <tf.Variable 'iter:0' shape=() dtype=int64, numpy=391>,\n",
      "  1: <tf.Variable 'iter/replica_1:0' shape=() dtype=int64, numpy=391>,\n",
      "  2: <tf.Variable 'iter/replica_2:0' shape=() dtype=int64, numpy=391>,\n",
      "  3: <tf.Variable 'iter/replica_3:0' shape=() dtype=int64, numpy=391>,\n",
      "  4: <tf.Variable 'iter/replica_4:0' shape=() dtype=int64, numpy=391>,\n",
      "  5: <tf.Variable 'iter/replica_5:0' shape=() dtype=int64, numpy=391>,\n",
      "  6: <tf.Variable 'iter/replica_6:0' shape=() dtype=int64, numpy=391>,\n",
      "  7: <tf.Variable 'iter/replica_7:0' shape=() dtype=int64, numpy=391>\n",
      "}\n",
      "391/391 [==============================] - 964s 2s/step - loss: 4.5529 - weighted_categorical_accuracy: 0.3885 - weighted_f1_score: 6.4686e-05 - val_loss: 0.3207 - val_weighted_categorical_accuracy: 0.1273 - val_weighted_f1_score: 4.5787e-05\n",
      "Epoch 3/10\n",
      "learning rate: tf.Tensor(4.441595e-06, shape=(), dtype=float32)iteration: MirroredVariable:{\n",
      "  0: <tf.Variable 'iter:0' shape=() dtype=int64, numpy=782>,\n",
      "  1: <tf.Variable 'iter/replica_1:0' shape=() dtype=int64, numpy=782>,\n",
      "  2: <tf.Variable 'iter/replica_2:0' shape=() dtype=int64, numpy=782>,\n",
      "  3: <tf.Variable 'iter/replica_3:0' shape=() dtype=int64, numpy=782>,\n",
      "  4: <tf.Variable 'iter/replica_4:0' shape=() dtype=int64, numpy=782>,\n",
      "  5: <tf.Variable 'iter/replica_5:0' shape=() dtype=int64, numpy=782>,\n",
      "  6: <tf.Variable 'iter/replica_6:0' shape=() dtype=int64, numpy=782>,\n",
      "  7: <tf.Variable 'iter/replica_7:0' shape=() dtype=int64, numpy=782>\n",
      "}\n",
      "391/391 [==============================] - 960s 2s/step - loss: 4.1393 - weighted_categorical_accuracy: 0.4968 - weighted_f1_score: 1.2391e-04 - val_loss: 0.2969 - val_weighted_categorical_accuracy: 0.1412 - val_weighted_f1_score: 1.4035e-04\n",
      "Epoch 4/10\n",
      "learning rate: tf.Tensor(3.884615e-06, shape=(), dtype=float32)iteration: MirroredVariable:{\n",
      "  0: <tf.Variable 'iter:0' shape=() dtype=int64, numpy=1173>,\n",
      "  1: <tf.Variable 'iter/replica_1:0' shape=() dtype=int64, numpy=1173>,\n",
      "  2: <tf.Variable 'iter/replica_2:0' shape=() dtype=int64, numpy=1173>,\n",
      "  3: <tf.Variable 'iter/replica_3:0' shape=() dtype=int64, numpy=1173>,\n",
      "  4: <tf.Variable 'iter/replica_4:0' shape=() dtype=int64, numpy=1173>,\n",
      "  5: <tf.Variable 'iter/replica_5:0' shape=() dtype=int64, numpy=1173>,\n",
      "  6: <tf.Variable 'iter/replica_6:0' shape=() dtype=int64, numpy=1173>,\n",
      "  7: <tf.Variable 'iter/replica_7:0' shape=() dtype=int64, numpy=1173>\n",
      "}\n",
      "391/391 [==============================] - 959s 2s/step - loss: 3.6413 - weighted_categorical_accuracy: 0.4984 - weighted_f1_score: 1.2872e-04 - val_loss: 0.3214 - val_weighted_categorical_accuracy: 0.1273 - val_weighted_f1_score: 4.6117e-05\n",
      "Epoch 5/10\n",
      "learning rate: tf.Tensor(3.3276353e-06, shape=(), dtype=float32)iteration: MirroredVariable:{\n",
      "  0: <tf.Variable 'iter:0' shape=() dtype=int64, numpy=1564>,\n",
      "  1: <tf.Variable 'iter/replica_1:0' shape=() dtype=int64, numpy=1564>,\n",
      "  2: <tf.Variable 'iter/replica_2:0' shape=() dtype=int64, numpy=1564>,\n",
      "  3: <tf.Variable 'iter/replica_3:0' shape=() dtype=int64, numpy=1564>,\n",
      "  4: <tf.Variable 'iter/replica_4:0' shape=() dtype=int64, numpy=1564>,\n",
      "  5: <tf.Variable 'iter/replica_5:0' shape=() dtype=int64, numpy=1564>,\n",
      "  6: <tf.Variable 'iter/replica_6:0' shape=() dtype=int64, numpy=1564>,\n",
      "  7: <tf.Variable 'iter/replica_7:0' shape=() dtype=int64, numpy=1564>\n",
      "}\n",
      "391/391 [==============================] - 961s 2s/step - loss: 3.8796 - weighted_categorical_accuracy: 0.4956 - weighted_f1_score: 1.4059e-04 - val_loss: 0.3242 - val_weighted_categorical_accuracy: 0.1273 - val_weighted_f1_score: 4.6407e-05\n",
      "Epoch 6/10\n",
      "learning rate: tf.Tensor(2.770655e-06, shape=(), dtype=float32)iteration: MirroredVariable:{\n",
      "  0: <tf.Variable 'iter:0' shape=() dtype=int64, numpy=1955>,\n",
      "  1: <tf.Variable 'iter/replica_1:0' shape=() dtype=int64, numpy=1955>,\n",
      "  2: <tf.Variable 'iter/replica_2:0' shape=() dtype=int64, numpy=1955>,\n",
      "  3: <tf.Variable 'iter/replica_3:0' shape=() dtype=int64, numpy=1955>,\n",
      "  4: <tf.Variable 'iter/replica_4:0' shape=() dtype=int64, numpy=1955>,\n",
      "  5: <tf.Variable 'iter/replica_5:0' shape=() dtype=int64, numpy=1955>,\n",
      "  6: <tf.Variable 'iter/replica_6:0' shape=() dtype=int64, numpy=1955>,\n",
      "  7: <tf.Variable 'iter/replica_7:0' shape=() dtype=int64, numpy=1955>\n",
      "}\n",
      "391/391 [==============================] - 961s 2s/step - loss: 3.3639 - weighted_categorical_accuracy: 0.5533 - weighted_f1_score: 1.3374e-04 - val_loss: 0.3046 - val_weighted_categorical_accuracy: 0.1256 - val_weighted_f1_score: 1.1612e-04\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'WarmUp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7b4aa1a7628c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msweep_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#wandb.agent(sweep_id, train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-3a2bf8ee30e7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                             callbacks=[WandbCallback(),\n\u001b[1;32m     61\u001b[0m                                        \u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLRLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                                        RemoveGarbaseCallback()])\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m           \u001b[0mold_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2543\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mold_lr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m             \u001b[0mnew_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'WarmUp'"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "  \"name\": \"MWSA Sweep\",\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"lr\": {\n",
    "            \"values\": [5e-5,5e-4,5e-6]\n",
    "        },\n",
    "        \"weight_decay_rate\":{\n",
    "            \"values\": [0.1,0.2,0.3]\n",
    "        },\n",
    "        \"batch\": {\n",
    "            \"values\": [32]\n",
    "        }\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "\n",
    "model = train()\n",
    "#wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91414c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label=labelMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a0b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the code utilized TFTrainer, doesn't work atm.\n",
    "#from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "#training_args = TFTrainingArguments(\n",
    "#    output_dir='./results',          # output directory\n",
    "#    num_train_epochs=3,              # total # of training epochs\n",
    "#    per_device_train_batch_size=16,  # batch size per device during training\n",
    "#    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "#    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#    weight_decay=0.01,               # strength of weight decay\n",
    "#    logging_dir='./logs',            # directory for storing logs\n",
    "#)\n",
    "#trainer = TFTrainer(\n",
    "#    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "#    args=training_args,                  # training arguments, defined above\n",
    "#   train_dataset=train_dataset,    # tensorflow_datasets training dataset\n",
    "#    eval_dataset=test_dataset       # tensorflow_datasets evaluation dataset\n",
    "#)\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a83399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "acc = history_dict['weighted_categorical_accuracy']\n",
    "val_acc = history_dict['val_weighted_categorical_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53602e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "test_result = model.evaluate(test_dataset.batch(32))\n",
    "print(model.metrics_names)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "metric = [CategoricalAccuracy(), F1Score(num_classes=5, average='weighted')]\n",
    "optimizer = create_optimizer(init_lr=wandb.config.lr, weight_de)\n",
    "#optimizer = AdamWeightDecay(learning_rate=wandb.config.lr, weight_decay_rate=wandb.config.weight_decay_rate)\n",
    "loaded_model.compile(optimizer=optimizer, loss=loss, metrics=metric, weighted_metrics=metric)\n",
    "best_result = loaded_model.evaluate(test_dataset.batch(32))\n",
    "print(loaded_model.metrics_names)\n",
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780afd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.element_spec\n",
    "for text, label in test_dataset.take(3):\n",
    "    print(text)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80f3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_path = os.path.join(\"mwsa\",\"mwsa_best\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8943bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d1db022c0c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.save_pretrained(save_path, saved_model=True)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_info()\n",
    "result = model.predict(test_dataset.batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64038999",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb3611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118acfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/mwsa/mwsa_best/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'mwsa/mwsa_best'. Make sure that:\n\n- 'mwsa/mwsa_best' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'mwsa/mwsa_best' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/mwsa/mwsa_best/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2e0e723310fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \u001b[0m_from_auto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_auto_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m             )\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             logger.warn(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             )\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'mwsa/mwsa_best'. Make sure that:\n\n- 'mwsa/mwsa_best' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'mwsa/mwsa_best' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "# In order to use GCP for trained models, we have to save the model as TF Saved_Model format.\n",
    "# Just calling model.save() doesn't work.\n",
    "# https://colab.research.google.com/drive/1p1Nifh1P-vqAZ1gHsNSCXAHzVWzl5YPP\n",
    "# https://github.com/huggingface/transformers/issues/2021\n",
    "import os \n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.evaluate(test_dataset.batch(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8523300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2caef7b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa09c070b5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloaded_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minput_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"only me\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"only me\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_path' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import tensorflow as tf\n",
    "\n",
    "loaded_tokenizer = BertTokenizerFast.from_pretrained(save_path)\n",
    "input_encodings = loaded_tokenizer([\"only me\"],[\"only me\"], max_length=max_length, truncation=True, padding='max_length', return_tensors=\"tf\")\n",
    "\n",
    "input_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ec2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(input_encodings)\n",
    "))\n",
    "\n",
    "print(data)\n",
    "\n",
    "for ids in input_encodings[\"input_ids\"]:\n",
    "  print(loaded_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#predict_input = (test_encodings,  truncation=True, max_length=128, padding='max_length')\n",
    "result = loaded_model.predict(test_dataset.batch(32))\n",
    "probabilities = tf.nn.softmax(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2208d73f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-c91096c1418f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c91096c1418f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    predict_input = (test_encodings,  truncation=True, max_length=max_length, padding='max_length')\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "predict_input = (test_encodings,  truncation=True, max_length=max_length, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2aadc0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([544, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7eadf3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(544, 5)\n",
      "[[-1.4968938  1.8233801  1.8028307 -2.201978  -1.6621366]\n",
      " [-1.3648084  2.5407703  1.3034997 -1.7457976 -1.8486904]\n",
      " [-0.9769792  1.4272964  2.137048  -2.0812771 -1.9733627]\n",
      " ...\n",
      " [-2.3338146  2.0951207  1.1570156 -0.8065253 -0.8023103]\n",
      " [-1.3284367  2.34798    1.6983447 -1.7060848 -1.7351846]\n",
      " [-1.3589617  3.1047041  0.8355413 -1.3977504 -1.0957118]]\n"
     ]
    }
   ],
   "source": [
    "print(result.logits.shape) #(128*batchlength, num of labels)\n",
    "print(result.logits) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54e04892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "105f9c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'broader', 1: 'exact', 2: 'narrower', 3: 'none', 4: 'related'}\n"
     ]
    }
   ],
   "source": [
    "#lst = range(5)\n",
    "labels = le.classes_\n",
    "labelMap = {v: k for v, k in enumerate(labels)}\n",
    "    \n",
    "print(labelMap)\n",
    "#>>> {k: v for v, k in enumerate(lst)}\n",
    "#{'A': 0, 'C': 2, 'B': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d6582a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.25      0.71      0.37        85\n",
      "           2       0.06      0.17      0.09        29\n",
      "           3       0.95      0.40      0.56       411\n",
      "           4       0.02      0.06      0.04        16\n",
      "\n",
      "    accuracy                           0.42       544\n",
      "   macro avg       0.26      0.27      0.21       544\n",
      "weighted avg       0.76      0.42      0.49       544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#import numpy\n",
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "numpy.set_printoptions(threshold=200)\n",
    "\n",
    "preds = tf.argmax(result.logits, axis=1).numpy()\n",
    "print(classification_report(np.argmax(test_labels,axis=1),preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbb3f068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(544, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f233d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.Series(preds)\n",
    "result_df = result_df.map(lambda res: le.inverse_transform([res])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff866dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig[4]=result_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dce98c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig.to_csv(\"bert_20210217\", sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, res in enumerate(preds):\n",
    "    print(test_data.loc[[ind]])\n",
    "    print(res)\n",
    "    print(le.inverse_transform([res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e761f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = list(range(0,test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f678e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "length = test_data.shape[0]\n",
    "\n",
    "print(test_data.loc[[index]])\n",
    "print('test_label' + str(test_labels[index]))\n",
    "print(probabilities[index*128])\n",
    "print('predicted: ' + str(le.inverse_transform([probabilities[index*128].numpy().argmax(axis=-1)])))\n",
    "print(le.inverse_transform([0]))\n",
    "print(le.inverse_transform([1]))\n",
    "print(le.inverse_transform([2]))\n",
    "print(le.inverse_transform([3]))\n",
    "print(le.inverse_transform([4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained(save_path, from_tf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fea28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950eee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m68"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
