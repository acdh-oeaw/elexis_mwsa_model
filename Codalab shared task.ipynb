{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Task\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\syim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "from spacy import displacy\n",
    "from enum import Enum\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "pd.set_option('display.max_colwidth',-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0            1                                     2  \\\n",
      "0    off        preposition  away from and no longer touching       \n",
      "1    off        preposition  away from and no longer touching       \n",
      "2    off        preposition  away from and no longer touching       \n",
      "3    off        preposition  in a position away from                \n",
      "4    off        preposition  in a position away from                \n",
      "..   ...                ...                      ...                \n",
      "551  on         preposition  immediately following                  \n",
      "552  one        number       the number 1                           \n",
      "553  one        number       the number 1                           \n",
      "554  offspring  noun         a person's child or an animal's baby   \n",
      "555  offspring  noun         a person's child or an animal's baby   \n",
      "\n",
      "                                                  3         4  \n",
      "0    away from; down from                            narrower  \n",
      "1    not wanting or allowed to have (food etc)       none      \n",
      "2    out of (a vehicle, train etc)                   none      \n",
      "3    away from; down from                            narrower  \n",
      "4    not wanting or allowed to have (food etc)       none      \n",
      "..                                         ...        ...      \n",
      "551  followed by                                     related   \n",
      "552  the number or figure 1                          exact     \n",
      "553  the age of 1                                    related   \n",
      "554  (formal, humorous) someone's child or children  broader   \n",
      "555  an animal's baby or babies                      none      \n",
      "\n",
      "[556 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "folder = '../Documents/ELEXIS/codalab/public_dat/train'\n",
    "\n",
    "#all_data = pd.read_csv('../Documents/ELEXIS/codalab/public_dat/train/english_kd.tsv',sep='\\t', header=None)\n",
    "all_data = pd.read_csv(folder + '/' + 'english_kd.tsv', sep='\\t', header=None)\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broader', 'narrower', 'none', 'related', 'exact'}\n"
     ]
    }
   ],
   "source": [
    "def add_column_names(df):\n",
    "    column_names = ['word','pos','def1','def2','relation']\n",
    "    df.columns = column_names\n",
    "    \n",
    "def load_data(file_path):\n",
    "    loaded_data = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    add_column_names(loaded_data)\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "def load_training_data(folder):\n",
    "    all_data = {}\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".tsv\"):\n",
    "            all_data[filename.split('.')[0]] = load_data(folder + '/' + filename)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def train_classifier(data):\n",
    "    for lang in data:\n",
    "        print(lang)\n",
    "        \n",
    "        analyze_by_class(data[lang])\n",
    "\n",
    "        train_set, test_set = prepare_data(data[lang])\n",
    "        \n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        \n",
    "        print(nltk.classify.accuracy(classifier, test_set))\n",
    "        classifier.show_most_informative_features(5)\n",
    "        print('\\n')\n",
    "\n",
    "def analyze_by_class(dataset):\n",
    "    separated = dict()\n",
    "    \n",
    "    for vector in dataset:\n",
    "        class_value = vector[1]\n",
    "        if (class_value not in separated):\n",
    "            separated[class_value] = list()\n",
    "        separated[class_value].append(vector[0])\n",
    "\n",
    "    for s in separated:\n",
    "        print(s, len(separated[s]))\n",
    "\n",
    "    return separated\n",
    "\n",
    "def prepare_data(dataset):\n",
    "    featuresets = [(find_features(row), label) for (row, label) in dataset]\n",
    "    f = int(len(featuresets) / 5)\n",
    "    print(f)\n",
    "    train_set, test_set = featuresets[f:], featuresets[:f]\n",
    "    print(len(train_set), len(test_set))\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "\n",
    "def find_features(row):\n",
    "    features = {}\n",
    "    features['first_word_same'] = (row['def1'].split(' ')[0].lower() == row['def2'].split(' ')[0].lower())\n",
    "    features['len difference'] = abs(len(row['def1'].split(' ')) - len(row['def2'].split(' ')[0]))\n",
    "\n",
    "    wordmatch = 0\n",
    "    for word in row['def1'].split(' ')[0].lower():\n",
    "        if word in row['def2'].lower():\n",
    "            wordmatch+=1\n",
    "\n",
    "    features['wordmatch'] = wordmatch\n",
    "\n",
    "    features['synsets'] = len(wn.synsets(row['lemma'])) #for specific pos e.g. wn.synsets('dog', pos=wn.VERB)\n",
    "\n",
    "    #if features['synsets'] == 0:\n",
    "    #    print('no synset for ',row['lemma']) TODO MULTILIGNUAL WN\n",
    "\n",
    "    return features\n",
    "\n",
    "all_data = load_training_data(folder)\n",
    "en_data = all_data['english_kd']\n",
    "print(set(en_data['relation']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Text Classifier to the pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print only narrower relations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word          pos  \\\n",
      "0    off       preposition   \n",
      "3    off       preposition   \n",
      "15   off       adverb        \n",
      "47   off       adverb        \n",
      "67   off       adverb        \n",
      "113  offer     verb          \n",
      "114  offer     verb          \n",
      "133  officer   adjective     \n",
      "138  official  adjective     \n",
      "143  offline   adjective     \n",
      "152  oil       noun          \n",
      "153  oil       noun          \n",
      "241  only      conjunction   \n",
      "242  onward    adverb        \n",
      "243  onward    adverb        \n",
      "244  opaque    adjective     \n",
      "245  opaque    adjective     \n",
      "319  open      verb          \n",
      "321  open      verb          \n",
      "323  open      verb          \n",
      "328  office    noun          \n",
      "338  oily      adjective     \n",
      "385  on        preposition   \n",
      "465  on        preposition   \n",
      "\n",
      "                                                                                      def1  \\\n",
      "0    away from and no longer touching                                                        \n",
      "3    in a position away from                                                                 \n",
      "15   away from a place                                                                       \n",
      "47   indicates how far sth is in time or distance                                            \n",
      "67   not correct or exact                                                                    \n",
      "113  to state you are willing to pay a particular amount of money                            \n",
      "114  to provide sth for people to buy                                                        \n",
      "133  a police officer                                                                        \n",
      "138  relating to the things connected with sb's job                                          \n",
      "143  not connected to a computer, or not connected to the Internet                           \n",
      "152  a thick liquid substance taken from plants or animals and used in cooking               \n",
      "153  a thick black liquid taken from under the ground                                        \n",
      "241  indicates the statement just made is not completely true                                \n",
      "242  continuing from a particular time, date, etc.                                           \n",
      "243  continuing in a forward direction                                                       \n",
      "244  not able to be seen through                                                             \n",
      "245  not having a clearly understood meaning                                                 \n",
      "319  to start doing business or providing a service each day, or make sth start doing this   \n",
      "321  to start doing business for the first time, or make sth start doing this                \n",
      "323  to start being available for use, or to make available                                  \n",
      "328  a building people work in                                                               \n",
      "338  covered in or containing oil, often in an unpleasant way                                \n",
      "385  in a large vehicle used by the public                                                   \n",
      "465  indicates how sth is broadcast, recorded, etc.                                          \n",
      "\n",
      "                                                                                                def2  \\\n",
      "0    away from; down from                                                                              \n",
      "3    away from; down from                                                                              \n",
      "15   away (from a place, time etc)                                                                     \n",
      "47   away (from a place, time etc)                                                                     \n",
      "67   not as good as usual, or as it should be                                                          \n",
      "113  to say that one is willing                                                                        \n",
      "114  to put forward (a gift, suggestion etc) for acceptance or refusal                                 \n",
      "133  a person who carries out a public duty                                                            \n",
      "138  of or concerning a position of authority                                                          \n",
      "143  using a computer, but not connected to the Internet                                               \n",
      "152  a usually thick liquid that will not mix with water, obtained from plants, animals and minerals   \n",
      "153  a usually thick liquid that will not mix with water, obtained from plants, animals and minerals   \n",
      "241  except that, but                                                                                  \n",
      "242  moving forward (in place or time)                                                                 \n",
      "243  moving forward (in place or time)                                                                 \n",
      "244  not transparent                                                                                   \n",
      "245  not transparent                                                                                   \n",
      "319  to begin                                                                                          \n",
      "321  to begin                                                                                          \n",
      "323  to begin                                                                                          \n",
      "328  a room or building used for a particular purpose                                                  \n",
      "338  of, like or covered with oil                                                                      \n",
      "385  in or into (a vehicle, train etc)                                                                 \n",
      "465  being carried by                                                                                  \n",
      "\n",
      "     relation  \n",
      "0    narrower  \n",
      "3    narrower  \n",
      "15   narrower  \n",
      "47   narrower  \n",
      "67   narrower  \n",
      "113  narrower  \n",
      "114  narrower  \n",
      "133  narrower  \n",
      "138  narrower  \n",
      "143  narrower  \n",
      "152  narrower  \n",
      "153  narrower  \n",
      "241  narrower  \n",
      "242  narrower  \n",
      "243  narrower  \n",
      "244  narrower  \n",
      "245  narrower  \n",
      "319  narrower  \n",
      "321  narrower  \n",
      "323  narrower  \n",
      "328  narrower  \n",
      "338  narrower  \n",
      "385  narrower  \n",
      "465  narrower  \n"
     ]
    }
   ],
   "source": [
    "#df['def1']=df['def1'].str.wrap(20)\n",
    "is_narrower = en_data['relation']=='narrower'\n",
    "print(en_data[is_narrower])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Spacy NLP Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77beaa57643e447cba707befe310b4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=556), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd4f652112442a49db2c737ca7366da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=556), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def spacyDocForVec(vec):\n",
    "    doc_list = []\n",
    "    \n",
    "    for doc in tqdm(vec):\n",
    "        pr = nlp(doc)\n",
    "        doc_list.append(pr)\n",
    "    \n",
    "    return doc_list\n",
    "    \n",
    "doc_list = spacyDocForVec(en_data['def1'])\n",
    "doc_list2 = spacyDocForVec(en_data['def2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['def1_nlp']=doc_list\n",
    "#df['def2_nlp']=doc_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "away | lemma: away | norm: away | pos: ADV | tag: RB | dep: ROOT | sentiment: 0.0\n",
      "from | lemma: from | norm: from | pos: ADP | tag: IN | dep: prep | sentiment: 0.0\n",
      "; | lemma: ; | norm: ; | pos: PUNCT | tag: : | dep: punct | sentiment: 0.0\n",
      "down | lemma: down | norm: down | pos: ADV | tag: RB | dep: advmod | sentiment: 0.0\n",
      "from | lemma: from | norm: from | pos: ADP | tag: IN | dep: prep | sentiment: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "away from; down from"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = doc_list2[0]\n",
    "for token in doc_list2[0]:\n",
    "    print(token.text, \"| lemma:\", token.lemma_, \"| norm:\" , token.norm_, \"| pos:\" ,token.pos_, \"| tag:\", token.tag_, \"| dep:\", token.dep_, \"| sentiment:\", token.sentiment)\n",
    "\n",
    "doc_list2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data\n",
    "frame = pd.DataFrame({'doc1': doc_list, 'doc2': doc_list2})\n",
    "en_data['doc1'] = frame['doc1']\n",
    "en_data['doc2'] = frame['doc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarities</th>\n",
       "      <th>first_word_same</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.105435</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.491229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.597624</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.118619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323444</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.118619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.976754</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.863840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051192</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.491229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>-0.417336</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>1.472181</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.236450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>0.885584</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.236450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>1.067770</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.863840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>1.595843</td>\n",
       "      <td>False</td>\n",
       "      <td>0.626602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>556 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     similarities  first_word_same    length\n",
       "0    1.105435      True            -0.491229\n",
       "1    0.597624      False           -0.118619\n",
       "2    0.323444      False           -0.118619\n",
       "3    0.976754      False           -0.863840\n",
       "4    0.051192      False           -0.491229\n",
       "..        ...        ...                 ...\n",
       "551 -0.417336      False            0.999212\n",
       "552  1.472181      True            -1.236450\n",
       "553  0.885584      True            -1.236450\n",
       "554  1.067770      False           -0.863840\n",
       "555  1.595843      False            0.626602\n",
       "\n",
       "[556 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similarityVector(row):\n",
    "    doc1 = row['doc1']\n",
    "    doc2 = row['doc2']\n",
    "    \n",
    "    return doc1.similarity(doc2)\n",
    "    \n",
    "def first_word_same(row):\n",
    "     return (row['def1'].split(' ')[0].lower() == row['def2'].split(' ')[0].lower())\n",
    "\n",
    "def difference_in_length(row):\n",
    "    return abs(len(row['def1'].split(' ')) - len(row['def2'].split(' ')[0]))\n",
    "\n",
    "\n",
    "features = pd.DataFrame()\n",
    "\n",
    "similarities = []\n",
    "first_word = []\n",
    "length = []\n",
    "\n",
    "for i, row in en_data.iterrows():\n",
    "    similarities.append(similarityVector(row))\n",
    "    first_word.append(first_word_same(row))\n",
    "    length.append(difference_in_length(row)) \n",
    "\n",
    "features['similarities'] = preprocessing.scale(similarities)\n",
    "features['first_word_same'] = first_word\n",
    "features['length'] = preprocessing.scale(length)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word', 'pos', 'def1', 'def2', 'relation', 'doc1', 'doc2'], dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#en_data['similarities'] = similarities\n",
    "labels = en_data['relation']\n",
    "\n",
    "en_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'exact' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none']\n",
      "0.7589285714285714\n",
      "['none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none']\n",
      "0.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'exact' 'exact' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'exact'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none' 'none'\n",
      " 'none' 'none']\n",
      "0.7679\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(features, labels, test_size = 0.2)\n",
    "\n",
    "LR = LogisticRegression().fit(X_train, y_train)\n",
    "print(LR.predict(X_valid))\n",
    "\n",
    "print(LR.score(X_valid, y_valid))\n",
    "\n",
    "## Linear kernal won't work very well, experiment with nonlinear ones.\n",
    "SVM = svm.LinearSVC()\n",
    "SVM.fit(X_train, y_train)\n",
    "print(SVM.predict(X_valid))\n",
    "print(round(SVM.score(X_valid,y_valid), 4))\n",
    "\n",
    "RF = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "RF.fit(X_train, y_train)\n",
    "print(RF.predict(X_valid))\n",
    "print(round(RF.score(X_valid,y_valid), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "        \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n",
    "    )\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "    \n",
    "textcat.add_label('related')\n",
    "textcat.add_label('exact')\n",
    "textcat.add_label('broader')\n",
    "textcat.add_label('narrower')\n",
    "textcat.add_label('none')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "## Experiment with NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(doc):\n",
    "        # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "#nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "#nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}