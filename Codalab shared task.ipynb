{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Task\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\syim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "from spacy import displacy\n",
    "from enum import Enum\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "pd.set_option('display.max_colwidth',-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0          1                                                 2  \\\n",
      "0     squall  verb       blow in a squall                                   \n",
      "1     squall  verb       make high-pitched, whiney noises                   \n",
      "2     squall  verb       utter a sudden loud cry                            \n",
      "3     tumid   adjective  abnormally distended especially by fluids or gas   \n",
      "4     tumid   adjective  abnormally distended especially by fluids or gas   \n",
      "...     ...         ...                                               ...   \n",
      "8332  offer   verb       mount or put up                                    \n",
      "8333  offer   verb       mount or put up                                    \n",
      "8334  offer   verb       mount or put up                                    \n",
      "8335  offer   verb       mount or put up                                    \n",
      "8336  offer   verb       mount or put up                                    \n",
      "\n",
      "                                                                                                                                                                  3  \\\n",
      "0     to cry out; to scream or cry violently, as a woman frightened, or a child in anger or distress; .                                                               \n",
      "1     to cry out; to scream or cry violently, as a woman frightened, or a child in anger or distress; .                                                               \n",
      "2     to cry out; to scream or cry violently, as a woman frightened, or a child in anger or distress; .                                                               \n",
      "3     swelled, enlarged, or distended;                                                                                                                                \n",
      "4     rising above the level; protuberant.                                                                                                                            \n",
      "...                                    ...                                                                                                                            \n",
      "8332  to make an attempt; to make an essay or a trial; -- used with .                                                                                                 \n",
      "8333  to present in words; to proffer; to make a proposal of; to suggest; .  with the infinitive as an objective: to make an offer; to declare one's willingness; .   \n",
      "8334  to attempt; to undertake.                                                                                                                                       \n",
      "8335  to bid, as a price, reward, or wages;                                                                                                                           \n",
      "8336  to put in opposition to; to manifest in an offensive way; to threaten;                                                                                          \n",
      "\n",
      "          4  \n",
      "0     none   \n",
      "1     none   \n",
      "2     exact  \n",
      "3     exact  \n",
      "4     none   \n",
      "...    ...   \n",
      "8332  none   \n",
      "8333  none   \n",
      "8334  none   \n",
      "8335  none   \n",
      "8336  none   \n",
      "\n",
      "[8337 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "folder = '../Documents/ELEXIS/codalab/public_dat/train'\n",
    "\n",
    "#all_data = pd.read_csv('../Documents/ELEXIS/codalab/public_dat/train/english_kd.tsv',sep='\\t', header=None)\n",
    "all_data = pd.read_csv(folder + '/' + 'english_nuig.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Define Feature Extraction related functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broader', 'narrower', 'none', 'related', 'exact'}\n"
     ]
    }
   ],
   "source": [
    "def add_column_names(df):\n",
    "    column_names = ['word','pos','def1','def2','relation']\n",
    "    df.columns = column_names\n",
    "    \n",
    "def load_data(file_path):\n",
    "    loaded_data = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    add_column_names(loaded_data)\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "def load_training_data(folder):\n",
    "    all_data = {}\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".tsv\"):\n",
    "            all_data[filename.split('.')[0]] = load_data(folder + '/' + filename)\n",
    "    \n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broader', 'narrower', 'none', 'related', 'exact'}\n"
     ]
    }
   ],
   "source": [
    "all_data = load_training_data(folder)\n",
    "en_data = all_data['english_kd']\n",
    "print(set(en_data['relation']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Add Text Classifier to the pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print only narrower relations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word          pos  \\\n",
      "0    off        preposition   \n",
      "3    off        preposition   \n",
      "8    off        preposition   \n",
      "13   off        preposition   \n",
      "15   off        adverb        \n",
      "..   ...           ...        \n",
      "530  on         preposition   \n",
      "551  on         preposition   \n",
      "552  one        number        \n",
      "553  one        number        \n",
      "554  offspring  noun          \n",
      "\n",
      "                                                    def1  \\\n",
      "0    away from and no longer touching                      \n",
      "3    in a position away from                               \n",
      "8    not inside a large vehicle used by the public         \n",
      "13   not eating or taking                                  \n",
      "15   away from a place                                     \n",
      "..                 ...                                     \n",
      "530  indicates sth or sb uses a type of food, fuel, etc.   \n",
      "551  immediately following                                 \n",
      "552  the number 1                                          \n",
      "553  the number 1                                          \n",
      "554  a person's child or an animal's baby                  \n",
      "\n",
      "                                               def2  relation  \n",
      "0    away from; down from                            narrower  \n",
      "3    away from; down from                            narrower  \n",
      "8    out of (a vehicle, train etc)                   exact     \n",
      "13   not wanting or allowed to have (food etc)       exact     \n",
      "15   away (from a place, time etc)                   narrower  \n",
      "..                             ...                        ...  \n",
      "530  receiving, taking                               exact     \n",
      "551  followed by                                     related   \n",
      "552  the number or figure 1                          exact     \n",
      "553  the age of 1                                    related   \n",
      "554  (formal, humorous) someone's child or children  broader   \n",
      "\n",
      "[108 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#df['def1']=df['def1'].str.wrap(20)\n",
    "is_narrower = en_data['relation']=='narrower'\n",
    "is_not_none = en_data['relation']!='none'\n",
    "\n",
    "print(en_data[is_not_none])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run Spacy NLP Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3517ad1b34eb40f0957c446307b8cea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=556), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed7606bbf0c44ed90abb6ce29ea1dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=556), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>def1</th>\n",
       "      <th>def2</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>(away, from, and, no, longer, touching)</td>\n",
       "      <td>(away, from, ;, down, from)</td>\n",
       "      <td>narrower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>(away, from, and, no, longer, touching)</td>\n",
       "      <td>(not, wanting, or, allowed, to, have, (, food, etc, ))</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>(away, from, and, no, longer, touching)</td>\n",
       "      <td>(out, of, (, a, vehicle, ,, train, etc, ))</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>(in, a, position, away, from)</td>\n",
       "      <td>(away, from, ;, down, from)</td>\n",
       "      <td>narrower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>(in, a, position, away, from)</td>\n",
       "      <td>(not, wanting, or, allowed, to, have, (, food, etc, ))</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>on</td>\n",
       "      <td>preposition</td>\n",
       "      <td>(immediately, following)</td>\n",
       "      <td>(followed, by)</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>one</td>\n",
       "      <td>number</td>\n",
       "      <td>(the, number, 1)</td>\n",
       "      <td>(the, number, or, figure, 1)</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>one</td>\n",
       "      <td>number</td>\n",
       "      <td>(the, number, 1)</td>\n",
       "      <td>(the, age, of, 1)</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>offspring</td>\n",
       "      <td>noun</td>\n",
       "      <td>(a, person, 's, child, or, an, animal, 's, baby)</td>\n",
       "      <td>((, formal, ,, humorous, ), someone, 's, child, or, children)</td>\n",
       "      <td>broader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>offspring</td>\n",
       "      <td>noun</td>\n",
       "      <td>(a, person, 's, child, or, an, animal, 's, baby)</td>\n",
       "      <td>(an, animal, 's, baby, or, babies)</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>556 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word          pos                                              def1  \\\n",
       "0    off        preposition  (away, from, and, no, longer, touching)            \n",
       "1    off        preposition  (away, from, and, no, longer, touching)            \n",
       "2    off        preposition  (away, from, and, no, longer, touching)            \n",
       "3    off        preposition  (in, a, position, away, from)                      \n",
       "4    off        preposition  (in, a, position, away, from)                      \n",
       "..   ...                ...                            ...                      \n",
       "551  on         preposition  (immediately, following)                           \n",
       "552  one        number       (the, number, 1)                                   \n",
       "553  one        number       (the, number, 1)                                   \n",
       "554  offspring  noun         (a, person, 's, child, or, an, animal, 's, baby)   \n",
       "555  offspring  noun         (a, person, 's, child, or, an, animal, 's, baby)   \n",
       "\n",
       "                                                              def2  relation  \n",
       "0    (away, from, ;, down, from)                                    narrower  \n",
       "1    (not, wanting, or, allowed, to, have, (, food, etc, ))         none      \n",
       "2    (out, of, (, a, vehicle, ,, train, etc, ))                     none      \n",
       "3    (away, from, ;, down, from)                                    narrower  \n",
       "4    (not, wanting, or, allowed, to, have, (, food, etc, ))         none      \n",
       "..                                                      ...          ...      \n",
       "551  (followed, by)                                                 related   \n",
       "552  (the, number, or, figure, 1)                                   exact     \n",
       "553  (the, age, of, 1)                                              related   \n",
       "554  ((, formal, ,, humorous, ), someone, 's, child, or, children)  broader   \n",
       "555  (an, animal, 's, baby, or, babies)                             none      \n",
       "\n",
       "[556 rows x 5 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacyDocForVec(vec):\n",
    "    doc_list = []\n",
    "    \n",
    "    for doc in tqdm(vec):\n",
    "        pr = nlp(doc)\n",
    "        doc_list.append(pr)\n",
    "    \n",
    "    return doc_list\n",
    "\n",
    "\n",
    "modDfObj = en_data.apply(lambda x: spacyDocForVec(x) if x.name in ['def1', 'def2'] else x)\n",
    "modDfObj\n",
    "#doc_list = spacyDocForVec(en_data['def1'])\n",
    "#doc_list2 = spacyDocForVec(en_data['def2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['def1_nlp']=doc_list\n",
    "#df['def2_nlp']=doc_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from and no longer touching'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modDfObj['def1'][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-229-d82e7d6497eb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-229-d82e7d6497eb>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for token in doc_list2[0]\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "t1 = doc_list2[0]\n",
    "for token in doc_list2[0]\n",
    "    print(token.text, \"| lemma:\", token.lemma_, \"| norm:\" , token.norm_, \"| pos:\" ,token.pos_, \"| tag:\", token.tag_, \"| dep:\", token.dep_, \"| sentiment:\", token.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      away from and no longer touching    \n",
       "1      away from and no longer touching    \n",
       "2      away from and no longer touching    \n",
       "3      in a position away from             \n",
       "4      in a position away from             \n",
       "                ...                        \n",
       "551    immediately following               \n",
       "552    the number 1                        \n",
       "553    the number 1                        \n",
       "554    a person's child or an animal's baby\n",
       "555    a person's child or an animal's baby\n",
       "Name: def1, Length: 556, dtype: object"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_data\n",
    "frame = pd.DataFrame({'doc1': doc_list, 'doc2': doc_list2})\n",
    "en_data['doc1'] = frame['doc1']\n",
    "en_data['doc1'] = frame['doc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence2vec(row):\n",
    "    return row['def1'].similarity(row['def2'])\n",
    "\n",
    "    \n",
    "def first_word_same(row):\n",
    "     return (row['def1'].split(' ')[0].lower() == row['def2'].split(' ')[0].lower())\n",
    "\n",
    "def difference_in_length(row):\n",
    "    return abs(len(row['def1'].split(' ')) - len(row['def2'].split(' ')[0]))\n",
    "\n",
    "\n",
    "features = pd.DataFrame()\n",
    "\n",
    "similarities = []\n",
    "first_word = []\n",
    "length = []\n",
    "\n",
    "features['similarities'] = modDfObj.apply(lambda row: sentence2vec(row), axis = 1)\n",
    "\n",
    "#for i, row in en_data.iterrows():\n",
    "#    similarities.append(sentence2Vec(row))\n",
    "#    first_word.append(first_word_same(row))\n",
    "#    length.append(difference_in_length(row)) \n",
    "\n",
    "#features['similarities'] = similarities\n",
    "#features['first_word_same'] = first_word\n",
    "#features['length'] = length\n",
    "#Add Token Count using CountVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#en_data['similarities'] = similarities\n",
    "labels = en_data['relation']\n",
    "features['relation'] = en_data['relation']\n",
    "\n",
    "features_nltk = []\n",
    "for index, row in features.iterrows():\n",
    "    features_nltk.append((row.similarities, row.relation))\n",
    "    \n",
    "features_nltk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Split Dataset**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2)\n",
    "\n",
    "X_train_scaled = X_train.copy(deep = True)\n",
    "X_test_scaled = X_test.copy(deep = True)\n",
    "\n",
    "X_train_scaled['similarities'] = preprocessing.scale(X_train['similarities'])\n",
    "X_train_scaled['length'] = preprocessing.scale(X_train['length'])\n",
    "X_test_scaled['similarities'] = preprocessing.scale(X_test['similarities'])\n",
    "X_test_scaled['length'] = preprocessing.scale(X_test['length'])\n",
    "\n",
    "X_train_set = {}\n",
    "X_train_set['unscaled'] = X_train\n",
    "X_train_set['scaled'] = X_train_scaled\n",
    "\n",
    "X_test_set = {}\n",
    "X_test_set['unscaled'] = X_test\n",
    "X_test_set['scaled'] = X_test_scaled\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_baseline(test_set):\n",
    "    TP = 0\n",
    "    for index in test_set.index:\n",
    "        if test_set[index]=='none':\n",
    "            TP+=1\n",
    "\n",
    "    return float(TP/len(test_set))\n",
    "\n",
    "models = {}\n",
    "models['unscaled'] = [LR, SVM, RF]\n",
    "models['scaled'] = [LR_scaled, SVM_scaled, RF_scaled]\n",
    "\n",
    "def run_cv_with_dataset(model, trainset, y_train):\n",
    "    scores = cross_val_score(model, trainset, y_train, cv = 5)        \n",
    "    print('Cross validation scores for model' + model.__class__.__name__)\n",
    "    scores\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "def cross_val_models(models, all_training_set, y_train):\n",
    "    for estimator in models['unscaled']:\n",
    "        run_cv_with_dataset(estimator, all_training_set['unscaled'], y_train)\n",
    "    \n",
    "    for estimator in models['scaled']:\n",
    "        run_cv_with_dataset(estimator, all_training_set['scaled'], y_train)\n",
    "\n",
    "def compare_on_testset(models, testset_x, testset_y, testset_x_scaled):\n",
    "    print('Model Evaluation on Testset: ' + '\\n')\n",
    "    print('\\t' + 'BASELINE: ' + str(get_baseline(testset_y)) + '\\n')\n",
    "    \n",
    "    for estimator in models.scaled:\n",
    "        estimator.predict(testset_x)\n",
    "        print('\\t' + estimator.__class__.__name__)\n",
    "        score = estimator.score(testset_x, testset_y)\n",
    "        print('\\t\\t' + \"Accuracy: %0.4f (+/- %0.4f)\" % (score.mean(), score.std() * 2) + '\\n')\n",
    "        \n",
    "        estimator.predict(testset_x_scaled)\n",
    "        score = estimator.score(testset_x_scaled, testset_y)\n",
    "        print('\\t\\t' + \"Accuracy for scaled featureset: %0.4f (+/- %0.4f)\" % (score.mean(), score.std() * 2) + '\\n')\n",
    "\n",
    "def train_models(X_train, y_train, X_train_scaled):\n",
    "    print('baseline: ', str(get_baseline(y_test)) + '\\n')\n",
    "\n",
    "    LR = LogisticRegression( solver = 'lbfgs', multi_class = 'multinomial').fit(X_train, y_train)\n",
    "    LR_scaled = LogisticRegression( solver = 'lbfgs', multi_class = 'multinomial').fit(X_train_scaled, y_train)\n",
    "\n",
    "    ## Linear kernal won't work very well, experiment with nonlinear ones.\n",
    "    SVM = svm.LinearSVC(C=10.0).fit(X_train, y_train)\n",
    "    SVM_scaled = svm.LinearSVC().fit(X_train_scaled, y_train)\n",
    "\n",
    "    RF = RandomForestClassifier(max_depth = 5, random_state=0).fit(X_train, y_train)\n",
    "    RF_scaled = RandomForestClassifier(max_depth = 5, random_state=0).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    models = {}\n",
    "    models['unscaled'] = [LR, SVM, RF]\n",
    "    models['scaled'] = [LR_scaled, SVM_scaled, RF_scaled]\n",
    "    \n",
    "    return models\n",
    "\n",
    "models = train_models(X_train, y_train, X_train_scaled)\n",
    "cross_val_models(models, X_train_set, y_train)\n",
    "#compare_on_testset(models, X_test, y_test, X_test_scaled)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>def1</th>\n",
       "      <th>def2</th>\n",
       "      <th>relation</th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>away from and no longer touching</td>\n",
       "      <td>away from; down from</td>\n",
       "      <td>narrower</td>\n",
       "      <td>(blow, in, a, squall)</td>\n",
       "      <td>(to, cry, out, ;, to, scream, or, cry, violently, ,, as, a, woman, frightened, ,, or, a, child, in, anger, or, distress, ;, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>away from and no longer touching</td>\n",
       "      <td>not wanting or allowed to have (food etc)</td>\n",
       "      <td>none</td>\n",
       "      <td>(make, high, -, pitched, ,, whiney, noises)</td>\n",
       "      <td>(to, cry, out, ;, to, scream, or, cry, violently, ,, as, a, woman, frightened, ,, or, a, child, in, anger, or, distress, ;, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>away from and no longer touching</td>\n",
       "      <td>out of (a vehicle, train etc)</td>\n",
       "      <td>none</td>\n",
       "      <td>(utter, a, sudden, loud, cry)</td>\n",
       "      <td>(to, cry, out, ;, to, scream, or, cry, violently, ,, as, a, woman, frightened, ,, or, a, child, in, anger, or, distress, ;, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>in a position away from</td>\n",
       "      <td>away from; down from</td>\n",
       "      <td>narrower</td>\n",
       "      <td>(abnormally, distended, especially, by, fluids, or, gas)</td>\n",
       "      <td>(swelled, ,, enlarged, ,, or, distended, ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>off</td>\n",
       "      <td>preposition</td>\n",
       "      <td>in a position away from</td>\n",
       "      <td>not wanting or allowed to have (food etc)</td>\n",
       "      <td>none</td>\n",
       "      <td>(abnormally, distended, especially, by, fluids, or, gas)</td>\n",
       "      <td>(rising, above, the, level, ;, protuberant, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>on</td>\n",
       "      <td>preposition</td>\n",
       "      <td>immediately following</td>\n",
       "      <td>followed by</td>\n",
       "      <td>related</td>\n",
       "      <td>(the, act, of, rendering, a, person, legitimate)</td>\n",
       "      <td>(the, act, of, making, legitimate, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>one</td>\n",
       "      <td>number</td>\n",
       "      <td>the number 1</td>\n",
       "      <td>the number or figure 1</td>\n",
       "      <td>exact</td>\n",
       "      <td>(the, act, of, rendering, a, person, legitimate)</td>\n",
       "      <td>(lawful, birth, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>one</td>\n",
       "      <td>number</td>\n",
       "      <td>the number 1</td>\n",
       "      <td>the age of 1</td>\n",
       "      <td>related</td>\n",
       "      <td>(having, services, engaged, for, a, fee)</td>\n",
       "      <td>(performing, work, for, pay, ;, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>offspring</td>\n",
       "      <td>noun</td>\n",
       "      <td>a person's child or an animal's baby</td>\n",
       "      <td>(formal, humorous) someone's child or children</td>\n",
       "      <td>broader</td>\n",
       "      <td>(hired, for, the, exclusive, temporary, use, of, a, group, of, travelers)</td>\n",
       "      <td>(performing, work, for, pay, ;, .)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>offspring</td>\n",
       "      <td>noun</td>\n",
       "      <td>a person's child or an animal's baby</td>\n",
       "      <td>an animal's baby or babies</td>\n",
       "      <td>none</td>\n",
       "      <td>(a, bundle, of, fibers, (, especially, nerve, fibers, ))</td>\n",
       "      <td>(a, small, bundle, or, collection, ;, a, compact, cluster, ;, .)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>556 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word          pos                                  def1  \\\n",
       "0    off        preposition  away from and no longer touching       \n",
       "1    off        preposition  away from and no longer touching       \n",
       "2    off        preposition  away from and no longer touching       \n",
       "3    off        preposition  in a position away from                \n",
       "4    off        preposition  in a position away from                \n",
       "..   ...                ...                      ...                \n",
       "551  on         preposition  immediately following                  \n",
       "552  one        number       the number 1                           \n",
       "553  one        number       the number 1                           \n",
       "554  offspring  noun         a person's child or an animal's baby   \n",
       "555  offspring  noun         a person's child or an animal's baby   \n",
       "\n",
       "                                               def2  relation  \\\n",
       "0    away from; down from                            narrower   \n",
       "1    not wanting or allowed to have (food etc)       none       \n",
       "2    out of (a vehicle, train etc)                   none       \n",
       "3    away from; down from                            narrower   \n",
       "4    not wanting or allowed to have (food etc)       none       \n",
       "..                                         ...        ...       \n",
       "551  followed by                                     related    \n",
       "552  the number or figure 1                          exact      \n",
       "553  the age of 1                                    related    \n",
       "554  (formal, humorous) someone's child or children  broader    \n",
       "555  an animal's baby or babies                      none       \n",
       "\n",
       "                                                                          doc1  \\\n",
       "0    (blow, in, a, squall)                                                       \n",
       "1    (make, high, -, pitched, ,, whiney, noises)                                 \n",
       "2    (utter, a, sudden, loud, cry)                                               \n",
       "3    (abnormally, distended, especially, by, fluids, or, gas)                    \n",
       "4    (abnormally, distended, especially, by, fluids, or, gas)                    \n",
       "..                                                        ...                    \n",
       "551  (the, act, of, rendering, a, person, legitimate)                            \n",
       "552  (the, act, of, rendering, a, person, legitimate)                            \n",
       "553  (having, services, engaged, for, a, fee)                                    \n",
       "554  (hired, for, the, exclusive, temporary, use, of, a, group, of, travelers)   \n",
       "555  (a, bundle, of, fibers, (, especially, nerve, fibers, ))                    \n",
       "\n",
       "                                                                                                                               doc2  \n",
       "0    (to, cry, out, ;, to, scream, or, cry, violently, ,, as, a, woman, frightened, ,, or, a, child, in, anger, or, distress, ;, .)  \n",
       "1    (to, cry, out, ;, to, scream, or, cry, violently, ,, as, a, woman, frightened, ,, or, a, child, in, anger, or, distress, ;, .)  \n",
       "2    (to, cry, out, ;, to, scream, or, cry, violently, ,, as, a, woman, frightened, ,, or, a, child, in, anger, or, distress, ;, .)  \n",
       "3    (swelled, ,, enlarged, ,, or, distended, ;)                                                                                     \n",
       "4    (rising, above, the, level, ;, protuberant, .)                                                                                  \n",
       "..                                              ...                                                                                  \n",
       "551  (the, act, of, making, legitimate, .)                                                                                           \n",
       "552  (lawful, birth, .)                                                                                                              \n",
       "553  (performing, work, for, pay, ;, .)                                                                                              \n",
       "554  (performing, work, for, pay, ;, .)                                                                                              \n",
       "555  (a, small, bundle, or, collection, ;, a, compact, cluster, ;, .)                                                                \n",
       "\n",
       "[556 rows x 7 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "        \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n",
    "    )\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "    \n",
    "textcat.add_label('related')\n",
    "textcat.add_label('exact')\n",
    "textcat.add_label('broader')\n",
    "textcat.add_label('narrower')\n",
    "textcat.add_label('none')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(row):\n",
    "    return row['def1'].similarity(row['def2'])\n",
    "\n",
    "    \n",
    "def first_word_same(row):\n",
    "     return (row['def1'].split(' ')[0].lower() == row['def2'].split(' ')[0].lower())\n",
    "\n",
    "def difference_in_length(row):\n",
    "    return abs(len(row['def1'].split(' ')) - len(row['def2'].split(' ')[0]))\n",
    "\n",
    "\n",
    "features = pd.DataFrame()\n",
    "\n",
    "similarities = []\n",
    "first_word = []\n",
    "length = []\n",
    "\n",
    "features['similarities'] = modDfObj.apply(lambda row: sentence2vec(row), axis = 1)\n",
    "\n",
    "#for i, row in en_data.iterrows():\n",
    "#    similarities.append(sentence2Vec(row))\n",
    "#    first_word.append(first_word_same(row))\n",
    "#    length.append(difference_in_length(row)) \n",
    "\n",
    "#features['similarities'] = similarities\n",
    "#features['first_word_same'] = first_word\n",
    "#features['length'] = length\n",
    "#Add Token Count using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.856918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.776428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.732969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.836521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.689816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>0.615553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.915048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>0.822070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>0.850948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.934649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>556 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     similarities\n",
       "0    0.856918    \n",
       "1    0.776428    \n",
       "2    0.732969    \n",
       "3    0.836521    \n",
       "4    0.689816    \n",
       "..        ...    \n",
       "551  0.615553    \n",
       "552  0.915048    \n",
       "553  0.822070    \n",
       "554  0.850948    \n",
       "555  0.934649    \n",
       "\n",
       "[556 rows x 1 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8569175341853327, 'narrower'),\n",
       " (0.7764277625570569, 'none'),\n",
       " (0.7329691885840767, 'none'),\n",
       " (0.8365211865023756, 'narrower'),\n",
       " (0.6898163824998144, 'none'),\n",
       " (0.7367931309525856, 'none'),\n",
       " (0.6803962102027121, 'none'),\n",
       " (0.7521427806527426, 'none'),\n",
       " (0.7952153810359776, 'exact'),\n",
       " (0.6636760581748231, 'none'),\n",
       " (0.5763708126399653, 'none'),\n",
       " (0.6069563878788341, 'none'),\n",
       " (0.6514210433286518, 'none'),\n",
       " (0.8525577768853777, 'exact'),\n",
       " (0.7123831075808771, 'none'),\n",
       " (0.8482650807614045, 'narrower'),\n",
       " (0.6902407678790385, 'none'),\n",
       " (0.6923127331787525, 'none'),\n",
       " (0.5812013030610397, 'none'),\n",
       " (0.756551017992658, 'none'),\n",
       " (0.5780646498032915, 'none'),\n",
       " (0.7950366069506826, 'none'),\n",
       " (0.2533819159008091, 'none'),\n",
       " (0.735824786706821, 'none'),\n",
       " (0.662430446505536, 'none'),\n",
       " (0.6013527165121311, 'none'),\n",
       " (0.6239238959155543, 'none'),\n",
       " (0.708547657928058, 'none'),\n",
       " (0.566483874075168, 'none'),\n",
       " (0.676458026050009, 'broader'),\n",
       " (0.33422665314116606, 'none'),\n",
       " (0.7805555764860433, 'none'),\n",
       " (0.8307872645724801, 'none'),\n",
       " (0.9262615184292651, 'broader'),\n",
       " (0.532611898954182, 'none'),\n",
       " (0.8513617071034972, 'none'),\n",
       " (0.595702317495316, 'none'),\n",
       " (0.7655898116507808, 'none'),\n",
       " (0.33527377573579964, 'none'),\n",
       " (0.6384702724195211, 'none'),\n",
       " (0.6395515992123325, 'none'),\n",
       " (0.6034456236931373, 'none'),\n",
       " (0.4415999469820729, 'none'),\n",
       " (0.6983669742614741, 'none'),\n",
       " (0.5075624873137411, 'none'),\n",
       " (0.6924497474792682, 'none'),\n",
       " (0.2217636318871282, 'none'),\n",
       " (0.7955615049498859, 'narrower'),\n",
       " (0.7988778711801041, 'none'),\n",
       " (0.7735604577788072, 'none'),\n",
       " (0.5897316852683283, 'none'),\n",
       " (0.8472793314235424, 'none'),\n",
       " (0.587729689330148, 'none'),\n",
       " (0.7972392089110188, 'none'),\n",
       " (0.2700599764203946, 'none'),\n",
       " (0.7547433464034813, 'none'),\n",
       " (0.8236888472982796, 'none'),\n",
       " (0.7881448498565385, 'none'),\n",
       " (0.6222753637681931, 'none'),\n",
       " (0.8494202361806671, 'none'),\n",
       " (0.5107231790444074, 'none'),\n",
       " (0.7408329206511111, 'none'),\n",
       " (0.3780638956511738, 'exact'),\n",
       " (0.6829461586819474, 'none'),\n",
       " (0.7647855756620632, 'none'),\n",
       " (0.7092471760536881, 'none'),\n",
       " (0.613479678268956, 'none'),\n",
       " (0.8266706607984866, 'narrower'),\n",
       " (0.5280479558905622, 'none'),\n",
       " (0.6891373680634104, 'none'),\n",
       " (0.30771249475780865, 'none'),\n",
       " (0.7576432605609905, 'none'),\n",
       " (0.8340259483223322, 'exact'),\n",
       " (0.7534125771437735, 'none'),\n",
       " (0.6104392167279109, 'none'),\n",
       " (0.8369509328957402, 'none'),\n",
       " (0.5498791092664573, 'none'),\n",
       " (0.8040068270140687, 'none'),\n",
       " (0.3256625662679993, 'none'),\n",
       " (0.7231894835438039, 'none'),\n",
       " (0.7386827169369593, 'none'),\n",
       " (0.7125149645553143, 'none'),\n",
       " (0.5794363533422142, 'none'),\n",
       " (0.8089469556549701, 'none'),\n",
       " (0.5160863075958105, 'none'),\n",
       " (0.752100077907494, 'none'),\n",
       " (0.2272771260071579, 'none'),\n",
       " (0.7474422231275011, 'none'),\n",
       " (0.8226883025451435, 'none'),\n",
       " (0.7691564755969105, 'none'),\n",
       " (0.6367179860032437, 'none'),\n",
       " (0.8741485966067756, 'none'),\n",
       " (0.579219423167119, 'none'),\n",
       " (0.7288276208637036, 'none'),\n",
       " (0.31973822462773266, 'none'),\n",
       " (0.9236730205294792, 'exact'),\n",
       " (0.7752869740050642, 'none'),\n",
       " (0.7317223377101124, 'none'),\n",
       " (0.6435130298498367, 'none'),\n",
       " (0.48428639824339503, 'exact'),\n",
       " (0.4919159782400065, 'none'),\n",
       " (0.6380996662090349, 'none'),\n",
       " (0.30654847254788686, 'none'),\n",
       " (0.27101305319866403, 'none'),\n",
       " (0.7078032229330284, 'none'),\n",
       " (0.4759668095420668, 'none'),\n",
       " (0.3143755409179456, 'none'),\n",
       " (0.7618004623450779, 'exact'),\n",
       " (0.7235949715089776, 'exact'),\n",
       " (0.8000429400872743, 'none'),\n",
       " (0.7812987058259314, 'none'),\n",
       " (0.9043225870772327, 'exact'),\n",
       " (0.8405393889613775, 'none'),\n",
       " (0.9028802882314558, 'narrower'),\n",
       " (0.7917842603720275, 'narrower'),\n",
       " (0.8122055321947518, 'none'),\n",
       " (0.7381581816774683, 'exact'),\n",
       " (0.7765099441819033, 'none'),\n",
       " (0.815110295690546, 'none'),\n",
       " (0.893794832505361, 'exact'),\n",
       " (0.6948862305053373, 'none'),\n",
       " (0.8330122339092463, 'related'),\n",
       " (0.7227588324847567, 'none'),\n",
       " (0.8575547759199456, 'related'),\n",
       " (0.7227588324847567, 'none'),\n",
       " (0.8575547759199456, 'related'),\n",
       " (0.6739426977544901, 'none'),\n",
       " (0.7678592930467031, 'none'),\n",
       " (0.8651946219215331, 'exact'),\n",
       " (0.8828783978440112, 'exact'),\n",
       " (0.8468899710079675, 'exact'),\n",
       " (0.7951516325637374, 'none'),\n",
       " (0.7298389639487953, 'none'),\n",
       " (0.7233863070397533, 'narrower'),\n",
       " (0.8613687738407827, 'none'),\n",
       " (0.8727333181298106, 'exact'),\n",
       " (0.7916595908661244, 'none'),\n",
       " (0.8647995831015258, 'none'),\n",
       " (0.8330409995662752, 'narrower'),\n",
       " (0.8005679346928429, 'none'),\n",
       " (0.8695397873715145, 'none'),\n",
       " (0.8337388947311184, 'related'),\n",
       " (0.8132733358485856, 'exact'),\n",
       " (0.9790124081779313, 'narrower'),\n",
       " (0.9207901444088697, 'exact'),\n",
       " (0.8620441129528943, 'none'),\n",
       " (0.7994043206909557, 'none'),\n",
       " (0.6990505331006305, 'none'),\n",
       " (0.905347640647384, 'exact'),\n",
       " (0.5733953855684811, 'broader'),\n",
       " (0.8411936227733002, 'exact'),\n",
       " (0.7879031751951875, 'none'),\n",
       " (0.944533525355989, 'narrower'),\n",
       " (0.8664968583279771, 'narrower'),\n",
       " (0.901805182146438, 'exact'),\n",
       " (0.7650825650744216, 'exact'),\n",
       " (0.8821306843971976, 'none'),\n",
       " (0.8975197170631483, 'none'),\n",
       " (0.8776458412809102, 'related'),\n",
       " (0.5423645437560387, 'none'),\n",
       " (0.5846834696907452, 'exact'),\n",
       " (0.4515631889778857, 'none'),\n",
       " (0.5097095409959428, 'none'),\n",
       " (0.5292734477254387, 'none'),\n",
       " (0.7753560477262207, 'exact'),\n",
       " (0.6628807969869561, 'none'),\n",
       " (0.5870965203417929, 'none'),\n",
       " (0.6487014844710361, 'none'),\n",
       " (0.6912416672854532, 'none'),\n",
       " (0.8777111982725354, 'none'),\n",
       " (0.8191313374920892, 'none'),\n",
       " (0.8804845732021894, 'exact'),\n",
       " (0.8327491208146557, 'none'),\n",
       " (0.8862279053776834, 'none'),\n",
       " (0.8105740297677158, 'none'),\n",
       " (0.8200166578403231, 'none'),\n",
       " (0.6523046305012379, 'none'),\n",
       " (0.6659566081278191, 'none'),\n",
       " (0.8138494659815138, 'exact'),\n",
       " (0.5143542547579534, 'none'),\n",
       " (0.5292304993457795, 'related'),\n",
       " (0.32125358212677296, 'none'),\n",
       " (0.3881780711210213, 'none'),\n",
       " (0.40646395874891944, 'none'),\n",
       " (0.5993308834590055, 'none'),\n",
       " (0.6922130077278196, 'related'),\n",
       " (0.3909455973530112, 'none'),\n",
       " (0.46766899253256, 'none'),\n",
       " (0.5095636201735305, 'none'),\n",
       " (0.8047064315232775, 'none'),\n",
       " (0.7968685789640705, 'none'),\n",
       " (0.6336700019039845, 'none'),\n",
       " (0.684732345341401, 'none'),\n",
       " (0.7279470511626464, 'none'),\n",
       " (0.8633870828858645, 'exact'),\n",
       " (0.86598349259978, 'none'),\n",
       " (0.8092988986476025, 'none'),\n",
       " (1.0, 'exact'),\n",
       " (0.7608509053184832, 'none'),\n",
       " (0.9429567723341596, 'none'),\n",
       " (0.5265392061948746, 'none'),\n",
       " (0.5837524435563868, 'none'),\n",
       " (0.5265392061948746, 'none'),\n",
       " (0.5837524435563868, 'none'),\n",
       " (0.6171394860494179, 'none'),\n",
       " (0.6417922974289962, 'none'),\n",
       " (0.8218435065557208, 'none'),\n",
       " (0.8507774836873161, 'none'),\n",
       " (0.6164584301257914, 'none'),\n",
       " (0.6776167306286619, 'none'),\n",
       " (0.9518264236773714, 'exact'),\n",
       " (0.8899943965275187, 'exact'),\n",
       " (0.8268347444762073, 'none'),\n",
       " (0.7179901336224798, 'none'),\n",
       " (0.7263810915835213, 'exact'),\n",
       " (0.8694322791250675, 'exact'),\n",
       " (0.80314644533116, 'none'),\n",
       " (0.8644690032238879, 'none'),\n",
       " (0.86552802127151, 'exact'),\n",
       " (0.8847706884666305, 'exact'),\n",
       " (0.9194204954137246, 'none'),\n",
       " (0.8638539464589962, 'none'),\n",
       " (0.9172037163870732, 'exact'),\n",
       " (1.0, 'exact'),\n",
       " (0.6801227362004596, 'none'),\n",
       " (0.7845744624077448, 'none'),\n",
       " (0.8799960797262869, 'none'),\n",
       " (0.797158720912096, 'none'),\n",
       " (0.7226210802618063, 'related'),\n",
       " (0.8230976650589349, 'none'),\n",
       " (0.7901521066821918, 'none'),\n",
       " (0.8472494039025035, 'none'),\n",
       " (0.6635209592658917, 'none'),\n",
       " (0.8089948351376123, 'none'),\n",
       " (0.8008254914821438, 'none'),\n",
       " (0.5507008023202703, 'none'),\n",
       " (0.5473737730418186, 'none'),\n",
       " (0.6118117782496535, 'none'),\n",
       " (0.5784503441229838, 'exact'),\n",
       " (0.9610419423130164, 'exact'),\n",
       " (0.8659808851802879, 'none'),\n",
       " (0.827440368398804, 'narrower'),\n",
       " (0.8305132087003513, 'narrower'),\n",
       " (0.7903822981765404, 'narrower'),\n",
       " (0.6695103068903825, 'narrower'),\n",
       " (0.6839827240248854, 'narrower'),\n",
       " (0.8070727130780768, 'exact'),\n",
       " (0.7530690023351401, 'none'),\n",
       " (0.5904988391825841, 'none'),\n",
       " (0.735108999238218, 'none'),\n",
       " (0.2415438828760611, 'none'),\n",
       " (0.7475092782282675, 'none'),\n",
       " (0.652975226308274, 'none'),\n",
       " (0.7655698245178555, 'none'),\n",
       " (0.7555327726841073, 'none'),\n",
       " (0.8583129673431851, 'exact'),\n",
       " (0.6142515050221867, 'none'),\n",
       " (0.20729004387838204, 'none'),\n",
       " (0.7171698974295588, 'none'),\n",
       " (0.7633482009238644, 'none'),\n",
       " (0.6105378887708468, 'related'),\n",
       " (0.6981984931306248, 'none'),\n",
       " (0.7461411026943833, 'none'),\n",
       " (0.5206089648002336, 'none'),\n",
       " (0.10132269809227401, 'none'),\n",
       " (0.6153472754199673, 'none'),\n",
       " (0.5822161227847608, 'none'),\n",
       " (0.7671492106822483, 'none'),\n",
       " (0.7443458149353387, 'none'),\n",
       " (0.696528440795844, 'none'),\n",
       " (0.6434337041903502, 'none'),\n",
       " (0.19898424955858926, 'none'),\n",
       " (0.7737841134728048, 'none'),\n",
       " (0.9083974585481618, 'exact'),\n",
       " (0.7031357665301561, 'none'),\n",
       " (0.7645776183884947, 'none'),\n",
       " (0.7279444189090601, 'none'),\n",
       " (0.6993457411924917, 'none'),\n",
       " (0.4094984248246473, 'exact'),\n",
       " (0.6761482932853404, 'none'),\n",
       " (0.6068726753853821, 'none'),\n",
       " (0.7662834734085393, 'none'),\n",
       " (0.7417816986877007, 'none'),\n",
       " (0.7714106652865627, 'none'),\n",
       " (0.6741561535573659, 'none'),\n",
       " (0.3057410596246483, 'none'),\n",
       " (0.7565368644154914, 'none'),\n",
       " (0.7707476225080383, 'none'),\n",
       " (0.7572891855729885, 'none'),\n",
       " (0.8798989331924101, 'none'),\n",
       " (0.6996428940760528, 'none'),\n",
       " (0.6853184591738105, 'none'),\n",
       " (0.20762599390078537, 'none'),\n",
       " (0.7650282208599741, 'related'),\n",
       " (0.624129841942586, 'none'),\n",
       " (0.7367170540980241, 'none'),\n",
       " (0.7309675454081856, 'none'),\n",
       " (0.6033201043781379, 'none'),\n",
       " (0.7317897145892079, 'none'),\n",
       " (0.2764570339661382, 'related'),\n",
       " (0.7535235149363582, 'none'),\n",
       " (0.7576733183467299, 'none'),\n",
       " (0.7440046190359447, 'none'),\n",
       " (0.7897946644322978, 'none'),\n",
       " (0.66759096173939, 'none'),\n",
       " (0.7094904138509895, 'none'),\n",
       " (0.22316362446569915, 'none'),\n",
       " (0.7772338232666998, 'exact'),\n",
       " (0.6769334040883561, 'none'),\n",
       " (0.7887511687820686, 'related'),\n",
       " (0.7812743526034487, 'none'),\n",
       " (0.7557372336561383, 'none'),\n",
       " (0.6396677432315235, 'none'),\n",
       " (0.19978892981865704, 'none'),\n",
       " (0.757227611675597, 'none'),\n",
       " (0.7806616243250238, 'none'),\n",
       " (0.9459121798550778, 'exact'),\n",
       " (0.8546160154289731, 'none'),\n",
       " (0.8753555597918311, 'none'),\n",
       " (0.7929303828051038, 'narrower'),\n",
       " (0.8655626882943579, 'none'),\n",
       " (0.8174548475619804, 'narrower'),\n",
       " (0.9077377336780534, 'none'),\n",
       " (0.7927176355980878, 'narrower'),\n",
       " (0.7430673204133472, 'none'),\n",
       " (0.710761583779704, 'none'),\n",
       " (0.9055365842732068, 'broader'),\n",
       " (0.8860631479958695, 'broader'),\n",
       " (0.8794188760343149, 'narrower'),\n",
       " (0.8451326221031907, 'none'),\n",
       " (0.877910385210216, 'none'),\n",
       " (0.8715530461389179, 'none'),\n",
       " (0.874707047817027, 'none'),\n",
       " (0.8689440051569517, 'none'),\n",
       " (0.8101642155961878, 'none'),\n",
       " (0.7979746410036346, 'none'),\n",
       " (0.7974798770281708, 'none'),\n",
       " (0.863000498308481, 'broader'),\n",
       " (0.9112514395801928, 'narrower'),\n",
       " (0.7667143139126252, 'none'),\n",
       " (0.5750398290075194, 'exact'),\n",
       " (0.7413169621607731, 'none'),\n",
       " (0.7102935966949614, 'none'),\n",
       " (0.6303008631630145, 'none'),\n",
       " (0.656038664703806, 'none'),\n",
       " (0.5622065771860015, 'none'),\n",
       " (0.710592737193594, 'none'),\n",
       " (0.7500374632669434, 'exact'),\n",
       " (0.7031971319505107, 'related'),\n",
       " (0.6571307606655965, 'none'),\n",
       " (0.6025193228164469, 'none'),\n",
       " (0.759236217668414, 'exact'),\n",
       " (0.6966187870047023, 'none'),\n",
       " (0.5860892412440584, 'none'),\n",
       " (0.6322036479391744, 'none'),\n",
       " (0.6103917554481667, 'none'),\n",
       " (0.7449634242645913, 'none'),\n",
       " (0.7804611139304597, 'none'),\n",
       " (0.6392796932059738, 'none'),\n",
       " (0.7404466089740838, 'exact'),\n",
       " (0.9134372450117338, 'exact'),\n",
       " (0.7665286028733507, 'related'),\n",
       " (0.7828304763099156, 'none'),\n",
       " (0.7214999176581504, 'related'),\n",
       " (0.618138574763311, 'none'),\n",
       " (0.5549499990128184, 'none'),\n",
       " (0.8365514534493216, 'exact'),\n",
       " (0.5252919664570127, 'none'),\n",
       " (0.8416145633313965, 'exact'),\n",
       " (0.6594572738661334, 'related'),\n",
       " (0.7901123813313387, 'exact'),\n",
       " (0.5967508332707588, 'none'),\n",
       " (0.6332754446113442, 'none'),\n",
       " (0.5293724032887461, 'none'),\n",
       " (0.7081831388433971, 'none'),\n",
       " (0.2962483633252454, 'none'),\n",
       " (0.4949531140152958, 'none'),\n",
       " (0.638647462284717, 'none'),\n",
       " (0.5524363642344836, 'none'),\n",
       " (0.5872913786258767, 'none'),\n",
       " (0.6262012509584229, 'none'),\n",
       " (0.5097138650665556, 'none'),\n",
       " (0.5710522522469419, 'none'),\n",
       " (0.3889678254794353, 'none'),\n",
       " (0.7658924522595644, 'none'),\n",
       " (0.8163335303790725, 'narrower'),\n",
       " (0.7859369981420926, 'none'),\n",
       " (0.5312613067131843, 'none'),\n",
       " (0.8353586066271806, 'none'),\n",
       " (0.5864604408124968, 'none'),\n",
       " (0.6457972064785359, 'none'),\n",
       " (0.8021837542258106, 'none'),\n",
       " (0.5018314716796349, 'none'),\n",
       " (0.6453863222448694, 'none'),\n",
       " (0.7740785470609193, 'none'),\n",
       " (0.7547984860356963, 'none'),\n",
       " (0.7260437889059548, 'none'),\n",
       " (0.6359214242508263, 'none'),\n",
       " (0.7208867573844516, 'none'),\n",
       " (0.6929113923252845, 'none'),\n",
       " (0.7562985050665818, 'none'),\n",
       " (0.569018463258128, 'exact'),\n",
       " (0.7849584827290204, 'none'),\n",
       " (0.4514493462298094, 'none'),\n",
       " (0.6087508734363094, 'none'),\n",
       " (0.7091682495753641, 'none'),\n",
       " (0.5072653927726508, 'none'),\n",
       " (0.5188395342654142, 'none'),\n",
       " (0.6871859890489768, 'none'),\n",
       " (0.6291255528720529, 'none'),\n",
       " (0.6601674810798402, 'none'),\n",
       " (0.49307247940291193, 'none'),\n",
       " (0.7616100313375046, 'none'),\n",
       " (0.7405214645105318, 'none'),\n",
       " (0.8026529905499311, 'none'),\n",
       " (0.609549812373831, 'none'),\n",
       " (0.8088184856249229, 'none'),\n",
       " (0.46401570598205416, 'none'),\n",
       " (0.6760522010650427, 'none'),\n",
       " (0.7174554347309031, 'none'),\n",
       " (0.5008338225863661, 'none'),\n",
       " (0.6340332415675487, 'none'),\n",
       " (0.7021453188629602, 'none'),\n",
       " (0.6710799275608406, 'none'),\n",
       " (0.8377606860907816, 'none'),\n",
       " (0.5077480654169685, 'none'),\n",
       " (0.7989349940965398, 'none'),\n",
       " (0.7841843241900761, 'none'),\n",
       " (0.8698897682770476, 'exact'),\n",
       " (0.6031997474203329, 'none'),\n",
       " (0.8371410626220355, 'none'),\n",
       " (0.4300891676832865, 'none'),\n",
       " (0.6381537061527399, 'none'),\n",
       " (0.7639497307074907, 'none'),\n",
       " (0.48308541711716446, 'none'),\n",
       " (0.6569552644084939, 'none'),\n",
       " (0.7218805327111311, 'none'),\n",
       " (0.636808161564649, 'none'),\n",
       " (0.7768579455229886, 'related'),\n",
       " (0.5381040346238493, 'none'),\n",
       " (0.8376859387826178, 'none'),\n",
       " (0.8406583795466607, 'none'),\n",
       " (0.8629781958710849, 'none'),\n",
       " (0.5660164612695588, 'none'),\n",
       " (0.8619264602681569, 'none'),\n",
       " (0.4294032712162471, 'none'),\n",
       " (0.6753050716069117, 'none'),\n",
       " (0.8533736583423118, 'related'),\n",
       " (0.5644528598523119, 'none'),\n",
       " (0.7861756463936406, 'none'),\n",
       " (0.6892177024432193, 'none'),\n",
       " (0.650667616250864, 'none'),\n",
       " (0.7807794395276504, 'none'),\n",
       " (0.5386343227269316, 'none'),\n",
       " (0.7634213132295624, 'none'),\n",
       " (0.773497085348479, 'none'),\n",
       " (0.8190292063602942, 'none'),\n",
       " (0.5216265180418908, 'none'),\n",
       " (0.6687763776559332, 'none'),\n",
       " (0.42627058872583434, 'none'),\n",
       " (0.6993770363294374, 'none'),\n",
       " (0.6301643697202556, 'none'),\n",
       " (0.4054137546740453, 'none'),\n",
       " (0.5761751643738908, 'none'),\n",
       " (0.5868474211065442, 'none'),\n",
       " (0.5960037149729297, 'narrower'),\n",
       " (0.8607391446263046, 'none'),\n",
       " (0.4834579105839098, 'none'),\n",
       " (0.6494391069434223, 'none'),\n",
       " (0.6256903488710379, 'none'),\n",
       " (0.667830326193051, 'none'),\n",
       " (0.5471724150679289, 'none'),\n",
       " (0.6619121813874409, 'none'),\n",
       " (0.41767936131608063, 'none'),\n",
       " (0.5253652002201075, 'none'),\n",
       " (0.6163565877398304, 'none'),\n",
       " (0.45061085124114625, 'none'),\n",
       " (0.5506804375995387, 'none'),\n",
       " (0.6352053004832349, 'none'),\n",
       " (0.6018458826130499, 'none'),\n",
       " (0.7340540128412288, 'none'),\n",
       " (0.48941868323087817, 'none'),\n",
       " (0.5386823824574918, 'none'),\n",
       " (0.47028986146872254, 'none'),\n",
       " (0.5280291997471274, 'none'),\n",
       " (0.38464129169933936, 'none'),\n",
       " (0.5089308697243862, 'none'),\n",
       " (0.3464743154759546, 'none'),\n",
       " (0.4761024725655213, 'none'),\n",
       " (0.5061592173696795, 'none'),\n",
       " (0.488179801713367, 'none'),\n",
       " (0.4722219621660292, 'related'),\n",
       " (0.4553080279038739, 'none'),\n",
       " (0.4944181658620303, 'none'),\n",
       " (0.4927049330163775, 'none'),\n",
       " (0.44720375541738366, 'none'),\n",
       " (0.7469467421013303, 'none'),\n",
       " (0.7465631818152305, 'none'),\n",
       " (0.8644828982096278, 'none'),\n",
       " (0.6632435413163597, 'none'),\n",
       " (0.8047709739539566, 'none'),\n",
       " (0.4558995578556876, 'none'),\n",
       " (0.7169472419693651, 'none'),\n",
       " (0.7623693027559039, 'none'),\n",
       " (0.49866464213552664, 'none'),\n",
       " (0.6026499549613594, 'none'),\n",
       " (0.7169275197617294, 'none'),\n",
       " (0.6689122010321525, 'none'),\n",
       " (0.83737748600107, 'none'),\n",
       " (0.5164259652738418, 'none'),\n",
       " (0.7046868486552579, 'none'),\n",
       " (0.6536027434623087, 'none'),\n",
       " (0.6613526569318535, 'none'),\n",
       " (0.4209820335599207, 'none'),\n",
       " (0.7505600385236592, 'none'),\n",
       " (0.45961698938826334, 'none'),\n",
       " (0.5577025793269922, 'none'),\n",
       " (0.5987817966284789, 'none'),\n",
       " (0.39541233623598654, 'none'),\n",
       " (0.5193115549628962, 'none'),\n",
       " (0.652812102963729, 'related'),\n",
       " (0.5945617533186208, 'none'),\n",
       " (0.6500495241534868, 'none'),\n",
       " (0.4567227927905495, 'none'),\n",
       " (0.8067467818131309, 'none'),\n",
       " (0.8337114779425682, 'none'),\n",
       " (0.821683816978423, 'none'),\n",
       " (0.5038910760825583, 'none'),\n",
       " (0.7382538601986578, 'none'),\n",
       " (0.37872100644644935, 'none'),\n",
       " (0.6650505412325247, 'exact'),\n",
       " (0.6488916750541083, 'none'),\n",
       " (0.4569133709202054, 'none'),\n",
       " (0.6340435728473659, 'none'),\n",
       " (0.6239882862145293, 'none'),\n",
       " (0.5583609932297108, 'none'),\n",
       " (0.8184135332107388, 'none'),\n",
       " (0.4282579932037626, 'none'),\n",
       " (0.6040390900464055, 'none'),\n",
       " (0.6049266676307647, 'none'),\n",
       " (0.6966309993642444, 'none'),\n",
       " (0.40761393378439686, 'none'),\n",
       " (0.6765870342093799, 'none'),\n",
       " (0.43402677713789045, 'none'),\n",
       " (0.5979207922917104, 'none'),\n",
       " (0.6078702014166609, 'none'),\n",
       " (0.4214350852528028, 'none'),\n",
       " (0.5314559815929242, 'none'),\n",
       " (0.527304883049886, 'none'),\n",
       " (0.5924829222740474, 'none'),\n",
       " (0.6226832258794721, 'none'),\n",
       " (0.6155530638192108, 'related'),\n",
       " (0.9150481033703041, 'exact'),\n",
       " (0.8220703863879062, 'related'),\n",
       " (0.8509475041461547, 'broader'),\n",
       " (0.9346488837620389, 'none')]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#en_data['similarities'] = similarities\n",
    "labels = en_data['relation']\n",
    "features['relation'] = en_data['relation']\n",
    "\n",
    "features_nltk = []\n",
    "for index, row in features.iterrows():\n",
    "    features_nltk.append((row.similarities, row.relation))\n",
    "    \n",
    "features_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2)\n",
    "\n",
    "X_train_scaled = X_train.copy(deep = True)\n",
    "X_test_scaled = X_test.copy(deep = True)\n",
    "\n",
    "X_train_scaled['similarities'] = preprocessing.scale(X_train['similarities'])\n",
    "X_train_scaled['length'] = preprocessing.scale(X_train['length'])\n",
    "X_test_scaled['similarities'] = preprocessing.scale(X_test['similarities'])\n",
    "X_test_scaled['length'] = preprocessing.scale(X_test['length'])\n",
    "\n",
    "X_train_set = {}\n",
    "X_train_set['unscaled'] = X_train\n",
    "X_train_set['scaled'] = X_train_scaled\n",
    "\n",
    "X_test_set = {}\n",
    "X_test_set['unscaled'] = X_test\n",
    "X_test_set['scaled'] = X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline:  0.8561151079136691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores for modelLR scaled\n",
      "Accuracy: 0.8585 (+/- 0.0020)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores for modelLinearSVC\n",
      "Accuracy: 0.8534 (+/- 0.0103)\n",
      "Cross validation scores for modelRandomForestClassifier\n",
      "Accuracy: 0.8625 (+/- 0.0087)\n",
      "Cross validation scores for modelLR scaled\n",
      "Accuracy: 0.8594 (+/- 0.0030)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores for modelLinearSVC\n",
      "Accuracy: 0.8561 (+/- 0.0013)\n",
      "Cross validation scores for modelRandomForestClassifier\n",
      "Accuracy: 0.8625 (+/- 0.0087)\n"
     ]
    }
   ],
   "source": [
    "def get_baseline(test_set):\n",
    "    TP = 0\n",
    "    for index in test_set.index:\n",
    "        if test_set[index]=='none':\n",
    "            TP+=1\n",
    "\n",
    "    return float(TP/len(test_set))\n",
    "\n",
    "models = {}\n",
    "models['unscaled'] = [LR, SVM, RF]\n",
    "models['scaled'] = [LR_scaled, SVM_scaled, RF_scaled]\n",
    "\n",
    "def run_cv_with_dataset(model, trainset, y_train):\n",
    "    scores = cross_val_score(model, trainset, y_train, cv = 5)        \n",
    "    print('Cross validation scores for model' + model.__class__.__name__)\n",
    "    scores\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "def cross_val_models(models, all_training_set, y_train):\n",
    "    for estimator in models['unscaled']:\n",
    "        run_cv_with_dataset(estimator, all_training_set['unscaled'], y_train)\n",
    "    \n",
    "    for estimator in models['scaled']:\n",
    "        run_cv_with_dataset(estimator, all_training_set['scaled'], y_train)\n",
    "\n",
    "def compare_on_testset(models, testset_x, testset_y, testset_x_scaled):\n",
    "    print('Model Evaluation on Testset: ' + '\\n')\n",
    "    print('\\t' + 'BASELINE: ' + str(get_baseline(testset_y)) + '\\n')\n",
    "    \n",
    "    for estimator in models.scaled:\n",
    "        estimator.predict(testset_x)\n",
    "        print('\\t' + estimator.__class__.__name__)\n",
    "        score = estimator.score(testset_x, testset_y)\n",
    "        print('\\t\\t' + \"Accuracy: %0.4f (+/- %0.4f)\" % (score.mean(), score.std() * 2) + '\\n')\n",
    "        \n",
    "        estimator.predict(testset_x_scaled)\n",
    "        score = estimator.score(testset_x_scaled, testset_y)\n",
    "        print('\\t\\t' + \"Accuracy for scaled featureset: %0.4f (+/- %0.4f)\" % (score.mean(), score.std() * 2) + '\\n')\n",
    "\n",
    "def train_models(X_train, y_train, X_train_scaled):\n",
    "    print('baseline: ', str(get_baseline(y_test)) + '\\n')\n",
    "\n",
    "    LR = LogisticRegression( solver = 'lbfgs', multi_class = 'multinomial').fit(X_train, y_train)\n",
    "    LR_scaled = LogisticRegression( solver = 'lbfgs', multi_class = 'multinomial').fit(X_train_scaled, y_train)\n",
    "\n",
    "    ## Linear kernal won't work very well, experiment with nonlinear ones.\n",
    "    SVM = svm.LinearSVC(C=10.0).fit(X_train, y_train)\n",
    "    SVM_scaled = svm.LinearSVC().fit(X_train_scaled, y_train)\n",
    "\n",
    "    RF = RandomForestClassifier(max_depth = 5, random_state=0).fit(X_train, y_train)\n",
    "    RF_scaled = RandomForestClassifier(max_depth = 5, random_state=0).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    models = {}\n",
    "    models['unscaled'] = [LR, SVM, RF]\n",
    "    models['scaled'] = [LR_scaled, SVM_scaled, RF_scaled]\n",
    "    \n",
    "    return models\n",
    "\n",
    "models = train_models(X_train, y_train, X_train_scaled)\n",
    "cross_val_models(models, X_train_set, y_train)\n",
    "#compare_on_testset(models, X_test, y_test, X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "        \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n",
    "    )\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "    \n",
    "textcat.add_label('related')\n",
    "textcat.add_label('exact')\n",
    "textcat.add_label('broader')\n",
    "textcat.add_label('narrower')\n",
    "textcat.add_label('none')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "## Experiment with NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(doc):\n",
    "        # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "#nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "#nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}